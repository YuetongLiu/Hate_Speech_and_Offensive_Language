{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to GitHub:\n",
    "\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import csv\n",
    "from liwc import LIWC\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  hate_speech  offensive_language  neither  class  \\\n",
      "0      3            0                   0        3      2   \n",
      "1      3            0                   3        0      1   \n",
      "2      3            0                   3        0      1   \n",
      "3      3            0                   2        1      1   \n",
      "4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "(24783, 6)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/labeled_data.csv\")\n",
    "df.drop(df.columns[[0]], axis=1,inplace=True)\n",
    "print(df.iloc[:5,:])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     mayasolovely woman shouldn complain cleaning ...\n",
      "1     mleew boy das coldyga dwn bad cuffin da hoe s...\n",
      "2     ukindofband dawg  sbabylife eve fuck bich sa ...\n",
      "3                  cgandeson vivabased look like anny \n",
      "4     shenikaobes shi hea migh ue migh fake bich ol...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "txt=df.iloc[:,5]\n",
    "tweet=txt.copy(deep=True)\n",
    "#tweet=pd.DataFrame(tweet)\n",
    "#transform to lower case\n",
    "tweet=tweet.str.lower()\n",
    "#remove punctuation\n",
    "remove = str.maketrans('','',string.punctuation) \n",
    "tweet = tweet.str.translate(remove)\n",
    "#word tokenize and remove stopwords\n",
    "#remove digit and excessive whitespace\n",
    "for i in range(tweet.shape[0]):\n",
    "    text=tweet[i]\n",
    "    text1=''.join([ch+\" \" for ch in text.split() if ch not in ' 123456789'])\n",
    "    text2=''.join([word+\" \" for word in text1.split() if word not in stopwords])\n",
    "    tweet[i]=text2\n",
    "info=re.compile('[0-9|rt]')\n",
    "tweet=tweet.apply(lambda x: info.sub('',x))\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do more process of replacing:\n",
    "    1) urls\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     mayasolovely woman shouldn complain cleaning ...\n",
      "1     mleew boy das coldyga dwn bad cuffin da hoe s...\n",
      "2     ukindofband dawg sbabylife eve fuck bich sa c...\n",
      "3                  cgandeson vivabased look like anny \n",
      "4     shenikaobes shi hea migh ue migh fake bich ol...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text_string):\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "for i in range(tweet.shape[0]):\n",
    "    text=tweet[i]\n",
    "    text1=preprocess(text)\n",
    "    tweet[i]=text1\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "## 1.LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LIWC on the whole df\n",
    "\n",
    "LIWC_list=[]\n",
    "for i in range(len(tweet)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(tweet[i].split())\n",
    "    LIWC_list.append(L.readable(labels))\n",
    "LIWC_list\n",
    "#update preprocessed text to original df\n",
    "df.tweet=tweet\n",
    "hateSpeech=df[df['hate_speech'] ==3][\"tweet\"].reset_index(drop=True)\n",
    "#it is the list of hate speech\n",
    "LIWC_list1=[]\n",
    "for i in range(len(hateSpeech)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(hateSpeech[i].split())\n",
    "    LIWC_list1.append(L.readable(labels))\n",
    "LIWC_list1\n",
    "#function that add dictionary\n",
    "def mergeDict(dict1, dict2):\n",
    "   ''' Merge dictionaries and keep values of common keys in list'''\n",
    "   dict3 = {**dict1, **dict2}\n",
    "   for key, value in dict3.items():\n",
    "       if key in dict1 and key in dict2:\n",
    "               dict3[key] = value +dict1[key]\n",
    "   return dict3\n",
    "#add those dic together\n",
    "hate_dic=LIWC_list1[0]\n",
    "for i in range(len(LIWC_list1)):\n",
    "    hate_dic=mergeDict(hate_dic, LIWC_list1[i])\n",
    "hate_dic.keys()\n",
    "#find hate_relevent key by analyzing hate speech\n",
    "relevent_key=[k for k, v in hate_dic.items() if v > 50]\n",
    "#build dataframe with relevent columns\n",
    "LIWC_df=pd.DataFrame(data=LIWC_list, index=None, columns=None, dtype=None, copy=False)[relevent_key]\n",
    "LIWC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_df = LIWC_df.fillna(0)\n",
    "LIWC_df.to_csv(\"../data/LIWC.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Skip-gram\n",
    "\n",
    "### Average len of tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.66953153371263\n"
     ]
    }
   ],
   "source": [
    "#average len of tweet\n",
    "avg=0\n",
    "num=[]\n",
    "x=[x for x in range(24783)]\n",
    "for n in range(tweet.shape[0]):\n",
    "    avg+=len(tweet[n])\n",
    "    num.append(len(tweet[n]))\n",
    "avg=avg/(tweet.shape[0])\n",
    "plt.bar(x,num)\n",
    "plt.show()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212672"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=[]\n",
    "for i in range(tweet.shape[0]):\n",
    "    w=tweet[i].replace('\\n','').rstrip()\n",
    "    w1=w.split(\" \")\n",
    "    word.append(w1)\n",
    "def flatten(seq):\n",
    "    s=str(seq).replace('[', '').replace(']', '') \n",
    "    return [eval(x) for x in s.split(',') if x.strip()] \n",
    "word=[x for x in flatten(word)]\n",
    "word=[x for x in word if x!='']\n",
    "len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the input data for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [mayasolovely, woman, shouldn, complain, clean...\n",
       "1    [mleew, boy, das, coldyga, dwn, bad, cuffin, d...\n",
       "2    [ukindofband, dawg, sbabylife, eve, fuck, bich...\n",
       "3             [cgandeson, vivabased, look, like, anny]\n",
       "4    [shenikaobes, shi, hea, migh, ue, migh, fake, ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tweet)):\n",
    "    doc=tweet[i]\n",
    "    doc=doc.split(\" \")\n",
    "    doc=[x for x in doc if x!='']\n",
    "    tweet[i]=doc\n",
    "tweet[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec by skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721048, 1063360)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "model = Word2Vec(size=100, workers=5,sg=1)  #dimension is 200, consider 5 words, using skip-gram\n",
    "model.build_vocab(tweet)\n",
    "model.train(tweet,total_examples = model.corpus_count,epochs = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-900bfdd4be2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mword_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "word_vec=[]\n",
    "#average each word vector to get the vector of a single tweet\n",
    "for i in range(len(tweet)):\n",
    "    sum1=0\n",
    "    for w in tweet[i]:\n",
    "        try:\n",
    "            sum1+=model[w]\n",
    "        except:\n",
    "            continue\n",
    "    avg=sum1/(len(tweet[i]))\n",
    "    avg=list(avg)\n",
    "    word_vec.append(avg)\n",
    "word_vec[:5]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 598 is out of bounds for axis 0 with size 598",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-0b609e3b24e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword_vec_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0marray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_vec_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_vec_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mword_vec_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 598 is out of bounds for axis 0 with size 598"
     ]
    }
   ],
   "source": [
    "word_vec_array=np.array(word_vec)\n",
    "word_vec_l=np.reshape(word_vec_array[0],(1,100))\n",
    "for i in range(len(tweet)):\n",
    "    array=np.reshape(word_vec_array[i],(1,100))\n",
    "    word_vec_l=np.r_[word_vec_l,array]\n",
    "word_vec_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_lt=word_vec_l\n",
    "word_vec_lt=np.delete(word_vec_lt,0,0)\n",
    "word_vec_lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save this array as csv\n",
    "\n",
    "f = open('../data/word2vec_skip.csv', 'w')\n",
    "a = csv.writer(f)\n",
    "a.writerows(word_vec_lt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('../data/word2vec_skip1.csv', word_vec_lt, delimiter = ',')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.LDA\n",
    "\n",
    "construct dic for LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tweet)\n",
    "corpus = [dictionary.doc2bow(words) for words in tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.38819085916417634\n",
      "\n",
      "Coherence Score:  0.42194391540606735\n",
      "\n",
      "Coherence Score:  0.43920979717599234\n",
      "\n",
      "Coherence Score:  0.41266646537218143\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 60, 70, 80]:\n",
    "    lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=i)\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=tweet, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, '0.200*\"man\" + 0.074*\"show\" + 0.043*\"eveyhing\" + 0.039*\"bich\" + 0.028*\"one\"')\n",
      "(68, '0.147*\"n\" + 0.121*\"us\" + 0.066*\"wo\" + 0.047*\"done\" + 0.043*\"bae\"')\n",
      "(55, '0.165*\"alk\" + 0.130*\"bou\" + 0.033*\"kid\" + 0.029*\"youve\" + 0.027*\"bich\"')\n",
      "(30, '0.122*\"bee\" + 0.108*\"yellow\" + 0.067*\"bich\" + 0.056*\"ied\" + 0.043*\"ask\"')\n",
      "(23, '0.134*\"fa\" + 0.128*\"new\" + 0.077*\"bich\" + 0.063*\"die\" + 0.046*\"ype\"')\n",
      "(13, '0.108*\"geing\" + 0.088*\"black\" + 0.072*\"nohing\" + 0.057*\"h\" + 0.051*\"kids\"')\n",
      "(67, '0.141*\"gil\" + 0.103*\"call\" + 0.087*\"wie\" + 0.044*\"like\" + 0.042*\"days\"')\n",
      "(10, '0.087*\"sa\" + 0.063*\"song\" + 0.061*\"had\" + 0.047*\"sick\" + 0.047*\"pussies\"')\n",
      "(37, '0.118*\"gonna\" + 0.056*\"fee\" + 0.043*\"im\" + 0.043*\"use\" + 0.037*\"somebody\"')\n",
      "(3, '0.113*\"lile\" + 0.110*\"bich\" + 0.096*\"always\" + 0.043*\"wish\" + 0.043*\"hai\"')\n",
      "(39, '0.077*\"hi\" + 0.068*\"niggah\" + 0.064*\"gon\" + 0.064*\"onigh\" + 0.062*\"back\"')\n",
      "(11, '0.107*\"alking\" + 0.079*\"well\" + 0.074*\"side\" + 0.041*\"ges\" + 0.037*\"calling\"')\n",
      "(8, '0.146*\"look\" + 0.116*\"like\" + 0.085*\"bich\" + 0.077*\"sop\" + 0.070*\"sill\"')\n",
      "(0, '0.157*\"cun\" + 0.047*\"away\" + 0.045*\"acing\" + 0.043*\"maybe\" + 0.037*\"jihadi\"')\n",
      "(31, '0.104*\"mad\" + 0.059*\"find\" + 0.041*\"body\" + 0.036*\"slow\" + 0.033*\"ick\"')\n",
      "(42, '0.077*\"dumb\" + 0.072*\"yea\" + 0.056*\"women\" + 0.055*\"ima\" + 0.052*\"ah\"')\n",
      "(2, '0.098*\"ca\" + 0.082*\"anohe\" + 0.069*\"looks\" + 0.066*\"like\" + 0.052*\"bich\"')\n",
      "(54, '0.233*\"know\" + 0.067*\"bich\" + 0.045*\"boke\" + 0.040*\"hings\" + 0.034*\"biches\"')\n",
      "(25, '0.090*\"house\" + 0.082*\"hope\" + 0.051*\"bich\" + 0.047*\"wife\" + 0.042*\"pop\"')\n",
      "(48, '0.094*\"bes\" + 0.085*\"soy\" + 0.079*\"wach\" + 0.067*\"hell\" + 0.065*\"young\"')\n",
      "(array([[0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 1.010879  , 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       ...,\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571]], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# lda model, num_topics is the number of topic\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=70)\n",
    "# print 5 words for each topic \n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic)\n",
    "# topic infer\n",
    "lda_infer= lda.inference(corpus)\n",
    "print(lda.inference(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 70)\n"
     ]
    }
   ],
   "source": [
    "print(lda_infer[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_infer = pd.DataFrame(lda_infer[0])\n",
    "lda_infer.to_csv(\"../data/lda_infer.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "def evaluation(real_labels, pred_labels):\n",
    "    f1_micro = f1_score(real_labels, pred_labels, average='micro')\n",
    "    f1_macro = f1_score(real_labels, pred_labels, average='macro')\n",
    "    f1_weighted = f1_score(real_labels, pred_labels, average='weighted')\n",
    "    #f1_binary = f1_score(real_labels, pred_labels, average='binary')\n",
    "    #f1_samples = f1_score(real_labels, pred_labels, average='samples')\n",
    "\n",
    "    micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(real_labels, pred_labels, average='micro')\n",
    "    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(real_labels, pred_labels, average='macro')\n",
    "    \n",
    "\n",
    "    report = classification_report(real_labels, pred_labels)\n",
    "\n",
    "    print('f1 micro: ',f1_micro)\n",
    "    print('f1 macro: ',f1_macro)\n",
    "    print('f1 weighted: ',f1_weighted)\n",
    "    #print('f1 binary: ',f1_binary)\n",
    "    #print('f1 samples: ',f1_samples)\n",
    "    print('micro p, micro r, micro f1:', micro_p, micro_r, micro_f1)\n",
    "    print('macro p, macro r, macro f1:', macro_p, macro_r, macro_f1)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24783, 100), (24783, 70), (24783, 17))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ski = pd.read_csv('../data/word2vec_skip2.csv',header=None)\n",
    "X_lda = pd.read_csv('../data/lda_infer.csv')\n",
    "X_liwc = pd.read_csv('../data/LIWC.csv')\n",
    "y = pd.read_csv('../data/labeled_data.csv')['class']\n",
    "X_ski.shape,X_lda.shape, X_liwc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_ski)\n",
    "X_ski = pd.DataFrame(scaler.transform(X_ski))\n",
    "scaler.fit(X_lda)\n",
    "X_lda = pd.DataFrame(scaler.transform(X_lda))\n",
    "scaler.fit(X_liwc)\n",
    "X_liwc = pd.DataFrame(scaler.transform(X_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.210364</td>\n",
       "      <td>-0.432598</td>\n",
       "      <td>-0.997330</td>\n",
       "      <td>0.870346</td>\n",
       "      <td>-1.034201</td>\n",
       "      <td>0.125785</td>\n",
       "      <td>-0.308096</td>\n",
       "      <td>0.768088</td>\n",
       "      <td>-0.112404</td>\n",
       "      <td>-0.485286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>0.934323</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970142</td>\n",
       "      <td>0.184729</td>\n",
       "      <td>0.762217</td>\n",
       "      <td>0.069552</td>\n",
       "      <td>0.504674</td>\n",
       "      <td>0.281666</td>\n",
       "      <td>-1.103539</td>\n",
       "      <td>-0.847527</td>\n",
       "      <td>0.553633</td>\n",
       "      <td>1.255813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>1.577765</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.507980</td>\n",
       "      <td>-0.775718</td>\n",
       "      <td>0.262399</td>\n",
       "      <td>0.749621</td>\n",
       "      <td>-0.418676</td>\n",
       "      <td>-0.965173</td>\n",
       "      <td>0.799103</td>\n",
       "      <td>0.480935</td>\n",
       "      <td>-0.230887</td>\n",
       "      <td>-0.661697</td>\n",
       "      <td>...</td>\n",
       "      <td>1.818742</td>\n",
       "      <td>3.010036</td>\n",
       "      <td>1.665324</td>\n",
       "      <td>0.934323</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.987364</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.718547</td>\n",
       "      <td>0.118731</td>\n",
       "      <td>-1.381494</td>\n",
       "      <td>-0.043071</td>\n",
       "      <td>0.471597</td>\n",
       "      <td>1.835291</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>0.099431</td>\n",
       "      <td>-1.110124</td>\n",
       "      <td>-0.801691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>1.577765</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>-0.577253</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.889593</td>\n",
       "      <td>-1.040487</td>\n",
       "      <td>1.200376</td>\n",
       "      <td>0.996471</td>\n",
       "      <td>-0.239872</td>\n",
       "      <td>-1.111058</td>\n",
       "      <td>0.795121</td>\n",
       "      <td>0.486174</td>\n",
       "      <td>0.017350</td>\n",
       "      <td>-0.311059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>0.439394</td>\n",
       "      <td>1.183971</td>\n",
       "      <td>-0.918551</td>\n",
       "      <td>-0.949504</td>\n",
       "      <td>0.048243</td>\n",
       "      <td>1.495339</td>\n",
       "      <td>-1.156798</td>\n",
       "      <td>-0.855282</td>\n",
       "      <td>0.710415</td>\n",
       "      <td>1.252661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>-0.577253</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>-0.237918</td>\n",
       "      <td>-1.377332</td>\n",
       "      <td>0.347184</td>\n",
       "      <td>1.690207</td>\n",
       "      <td>-1.265389</td>\n",
       "      <td>-0.788985</td>\n",
       "      <td>0.281229</td>\n",
       "      <td>1.067327</td>\n",
       "      <td>0.934023</td>\n",
       "      <td>0.176924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>-0.727257</td>\n",
       "      <td>-1.709331</td>\n",
       "      <td>1.590391</td>\n",
       "      <td>1.838453</td>\n",
       "      <td>1.139058</td>\n",
       "      <td>-1.406139</td>\n",
       "      <td>-0.940876</td>\n",
       "      <td>0.227202</td>\n",
       "      <td>-1.304643</td>\n",
       "      <td>0.536335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>1.665324</td>\n",
       "      <td>0.934323</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.987364</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>1.241618</td>\n",
       "      <td>0.058139</td>\n",
       "      <td>0.255205</td>\n",
       "      <td>-0.361494</td>\n",
       "      <td>0.767290</td>\n",
       "      <td>-0.042059</td>\n",
       "      <td>0.154172</td>\n",
       "      <td>0.396215</td>\n",
       "      <td>-1.199854</td>\n",
       "      <td>0.053228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>1.665324</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>-0.089751</td>\n",
       "      <td>1.726396</td>\n",
       "      <td>-1.457368</td>\n",
       "      <td>-1.187706</td>\n",
       "      <td>0.232446</td>\n",
       "      <td>1.920649</td>\n",
       "      <td>-1.809326</td>\n",
       "      <td>-1.150480</td>\n",
       "      <td>0.276188</td>\n",
       "      <td>0.791993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>3.594774</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>-0.577253</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.210364 -0.432598 -0.997330  0.870346 -1.034201  0.125785 -0.308096   \n",
       "1     -0.970142  0.184729  0.762217  0.069552  0.504674  0.281666 -1.103539   \n",
       "2     -0.507980 -0.775718  0.262399  0.749621 -0.418676 -0.965173  0.799103   \n",
       "3     -0.718547  0.118731 -1.381494 -0.043071  0.471597  1.835291 -1.708760   \n",
       "4     -0.889593 -1.040487  1.200376  0.996471 -0.239872 -1.111058  0.795121   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24778  0.439394  1.183971 -0.918551 -0.949504  0.048243  1.495339 -1.156798   \n",
       "24779 -0.237918 -1.377332  0.347184  1.690207 -1.265389 -0.788985  0.281229   \n",
       "24780 -0.727257 -1.709331  1.590391  1.838453  1.139058 -1.406139 -0.940876   \n",
       "24781  1.241618  0.058139  0.255205 -0.361494  0.767290 -0.042059  0.154172   \n",
       "24782 -0.089751  1.726396 -1.457368 -1.187706  0.232446  1.920649 -1.809326   \n",
       "\n",
       "             7         8         9   ...        7         8         9   \\\n",
       "0      0.768088 -0.112404 -0.485286  ...  0.556133  1.250531 -0.424595   \n",
       "1     -0.847527  0.553633  1.255813  ...  0.556133  1.250531 -0.424595   \n",
       "2      0.480935 -0.230887 -0.661697  ...  1.818742  3.010036  1.665324   \n",
       "3      0.099431 -1.110124 -0.801691  ... -0.706476 -0.508973 -0.424595   \n",
       "4      0.486174  0.017350 -0.311059  ...  0.556133  1.250531 -0.424595   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24778 -0.855282  0.710415  1.252661  ... -0.706476 -0.508973 -0.424595   \n",
       "24779  1.067327  0.934023  0.176924  ... -0.706476 -0.508973 -0.424595   \n",
       "24780  0.227202 -1.304643  0.536335  ...  0.556133  1.250531  1.665324   \n",
       "24781  0.396215 -1.199854  0.053228  ...  0.556133  1.250531  1.665324   \n",
       "24782 -1.150480  0.276188  0.791993  ... -0.706476 -0.508973 -0.424595   \n",
       "\n",
       "             10        11        12        13        14        15       16  \n",
       "0      0.934323 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "1     -0.562272 -0.660135 -0.555307  1.577765 -0.460441  0.893203 -0.49192  \n",
       "2      0.934323  0.622409  0.987364 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "3     -0.562272 -0.660135 -0.555307  1.577765 -0.460441 -0.577253 -0.49192  \n",
       "4     -0.562272 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "...         ...       ...       ...       ...       ...       ...      ...  \n",
       "24778 -0.562272 -0.660135 -0.555307 -0.439245 -0.460441 -0.577253 -0.49192  \n",
       "24779 -0.562272 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "24780  0.934323  0.622409  0.987364 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "24781 -0.562272 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "24782 -0.562272 -0.660135 -0.555307  3.594774 -0.460441 -0.577253 -0.49192  \n",
       "\n",
       "[24783 rows x 187 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda.columns = list(range(101,171))\n",
    "X_all = pd.concat([X_ski, X_lda, X_liwc], axis=1)\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train, X_all_test, y_train, y_test = train_test_split(X_all, y, random_state=17, test_size=0.1)\n",
    "X_ski_train, X_ski_test = X_all_train.iloc[:, list(range(0,100))], X_all_test.iloc[:, list(range(0,100))]\n",
    "X_lda_train, X_lda_test = X_all_train.iloc[:, list(range(100,170))], X_all_test.iloc[:, list(range(100,170))]\n",
    "X_liwc_train, X_liwc_test = X_all_train.iloc[:, list(range(170,187))], X_all_test.iloc[:, list(range(170,187))]\n",
    "X_liwc_lda_train, X_liwc_lda_test = pd.concat([X_lda_train, X_liwc_train], axis=1), pd.concat([X_lda_test, X_liwc_test], axis=1)\n",
    "X_ski_lda_train, X_ski_lda_test = pd.concat([X_lda_train, X_ski_train], axis=1), pd.concat([X_lda_test, X_ski_test], axis=1)\n",
    "X_liwc_ski_train, X_liwc_ski_test = pd.concat([X_ski_train, X_liwc_train], axis=1), pd.concat([X_ski_test, X_liwc_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8846308995562727\n",
      "f1 macro:  0.6409930966529657\n",
      "f1 weighted:  0.8991726366801739\n",
      "micro p, micro r, micro f1: 0.8846308995562727 0.8846308995562727 0.8846308995562727\n",
      "macro p, macro r, macro f1: 0.6353561160087069 0.7208947581600446 0.6409930966529657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.49      0.19        35\n",
      "           1       0.95      0.92      0.93      2010\n",
      "           2       0.84      0.76      0.79       434\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.64      0.72      0.64      2479\n",
      "weighted avg       0.92      0.88      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_all_train, y_train)\n",
    "y_preds = model.predict(X_all_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8830173457039129\n",
      "f1 macro:  0.6390490485413304\n",
      "f1 weighted:  0.8980547360428501\n",
      "micro p, micro r, micro f1: 0.8830173457039129 0.8830173457039129 0.8830173457039129\n",
      "macro p, macro r, macro f1: 0.6312973771712175 0.7387798107721718 0.6390490485413304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.55      0.20        31\n",
      "           1       0.95      0.92      0.93      2017\n",
      "           2       0.82      0.75      0.79       431\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.63      0.74      0.64      2479\n",
      "weighted avg       0.92      0.88      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_ski_train, y_train)\n",
    "y_preds = model.predict(X_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.7918515530455829\n",
      "f1 macro:  0.37432030499552615\n",
      "f1 weighted:  0.8528944220732593\n",
      "micro p, micro r, micro f1: 0.7918515530455829 0.7918515530455829 0.7918515530455829\n",
      "macro p, macro r, macro f1: 0.3775714552912366 0.44655208240567584 0.37432030499552615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.98      0.80      0.88      2364\n",
      "           2       0.15      0.54      0.24       114\n",
      "\n",
      "    accuracy                           0.79      2479\n",
      "   macro avg       0.38      0.45      0.37      2479\n",
      "weighted avg       0.94      0.79      0.85      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_lda_train, y_train)\n",
    "y_preds = model.predict(X_lda_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.7841871722468737\n",
      "f1 macro:  0.29301379154420076\n",
      "f1 weighted:  0.8786867794834967\n",
      "micro p, micro r, micro f1: 0.7841871722468737 0.7841871722468737 0.7841871722468737\n",
      "macro p, macro r, macro f1: 0.3331619537275064 0.26150121065375304 0.29301379154420076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.78      0.88      2478\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.78      2479\n",
      "   macro avg       0.33      0.26      0.29      2479\n",
      "weighted avg       1.00      0.78      0.88      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_train, y_train)\n",
    "y_preds = model.predict(X_liwc_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.7958854376764825\n",
      "f1 macro:  0.40876158511903316\n",
      "f1 weighted:  0.8436507391061131\n",
      "micro p, micro r, micro f1: 0.7958854376764825 0.7958854376764825 0.7958854376764825\n",
      "macro p, macro r, macro f1: 0.40424708241445156 0.45470788806052465 0.40876158511903316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.96      0.82      0.88      2296\n",
      "           2       0.25      0.55      0.34       179\n",
      "\n",
      "    accuracy                           0.80      2479\n",
      "   macro avg       0.40      0.45      0.41      2479\n",
      "weighted avg       0.91      0.80      0.84      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_lda_train, y_train)\n",
    "y_preds = model.predict(X_liwc_lda_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8838241226300928\n",
      "f1 macro:  0.6360190315776292\n",
      "f1 weighted:  0.8989315314304018\n",
      "micro p, micro r, micro f1: 0.8838241226300928 0.8838241226300928 0.8838241226300928\n",
      "macro p, macro r, macro f1: 0.6294305636077457 0.7159807019725203 0.6360190315776292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.47      0.18        34\n",
      "           1       0.95      0.92      0.93      2019\n",
      "           2       0.82      0.76      0.79       426\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.63      0.72      0.64      2479\n",
      "weighted avg       0.92      0.88      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_ski_lda_train, y_train)\n",
    "y_preds = model.predict(X_ski_lda_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8850342880193626\n",
      "f1 macro:  0.6454233200252131\n",
      "f1 weighted:  0.8993412883818996\n",
      "micro p, micro r, micro f1: 0.8850342880193627 0.8850342880193627 0.8850342880193626\n",
      "macro p, macro r, macro f1: 0.6404356447210048 0.7433920838534395 0.6454233200252131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.56      0.21        32\n",
      "           1       0.95      0.92      0.93      2001\n",
      "           2       0.85      0.75      0.79       446\n",
      "\n",
      "    accuracy                           0.89      2479\n",
      "   macro avg       0.64      0.74      0.65      2479\n",
      "weighted avg       0.92      0.89      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_ski_train, y_train)\n",
    "y_preds = model.predict(X_liwc_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems LIWC & Skip-Learn seems to be the best combination. \n",
    "Since the Coherence Score on LDA is lower than 0.5, \n",
    "it could be a good idea to remove lda features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22304, 61)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',\n",
    "                                            penalty=\"l1\",C=0.01, solver = 'saga', \n",
    "                                            max_iter = 2000)).fit(X_liwc_ski_train,y_train)\n",
    "#X_liwc_ski_train_ = select.fit_transform(X_liwc_ski_train,y_train)\n",
    "\n",
    "\n",
    "feature_idx = select.get_support()\n",
    "selected_columns = X_liwc_ski_train.columns[feature_idx]\n",
    "X_liwc_ski_train_ = X_liwc_ski_train[selected_columns]\n",
    "X_liwc_ski_test_ = X_liwc_ski_test[selected_columns]\n",
    "X_liwc_ski_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22304, 84)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = X_liwc_ski_train.corr()\n",
    "\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.3:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = X_liwc_ski_train.columns[columns]\n",
    "X_liwc_ski_train_f = X_liwc_ski_train[selected_columns]\n",
    "X_liwc_ski_test_f = X_liwc_ski_test[selected_columns]\n",
    "X_train_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8781766841468334\n",
      "f1 macro:  0.6231050690803706\n",
      "f1 weighted:  0.8938602955502181\n",
      "micro p, micro r, micro f1: 0.8781766841468334 0.8781766841468334 0.8781766841468334\n",
      "macro p, macro r, macro f1: 0.6226121035759181 0.7201859052757533 0.6231050690803706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.52      0.17        27\n",
      "           1       0.95      0.92      0.93      2005\n",
      "           2       0.82      0.72      0.77       447\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.62      0.72      0.62      2479\n",
      "weighted avg       0.91      0.88      0.89      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_ski_train_, y_train)\n",
    "y_preds = model.predict(X_liwc_ski_test_)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Model has better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8842275110931828\n",
      "f1 macro:  0.6105923159112313\n",
      "f1 weighted:  0.9035868503861484\n",
      "micro p, micro r, micro f1: 0.8842275110931828 0.8842275110931828 0.8842275110931828\n",
      "macro p, macro r, macro f1: 0.6173225136752598 0.7136861722478759 0.6105923159112313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.47      0.10        17\n",
      "           1       0.95      0.92      0.93      2022\n",
      "           2       0.84      0.75      0.80       440\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.62      0.71      0.61      2479\n",
      "weighted avg       0.93      0.88      0.90      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "model = svm.LinearSVC(max_iter = 2000).fit(X_liwc_ski_train, y_train)\n",
    "y_preds = model.predict(X_liwc_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression has better performance, and SVM doesn't converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, \n",
    "                           cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_liwc_ski_train, y_train)\n",
    "y_preds = grid_result.predict(X_liwc_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
