{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "def evaluation(real_labels, pred_labels):\n",
    "    f1_micro = f1_score(real_labels, pred_labels, average='micro')\n",
    "    f1_macro = f1_score(real_labels, pred_labels, average='macro')\n",
    "    f1_weighted = f1_score(real_labels, pred_labels, average='weighted')\n",
    "    #f1_binary = f1_score(real_labels, pred_labels, average='binary')\n",
    "    #f1_samples = f1_score(real_labels, pred_labels, average='samples')\n",
    "\n",
    "    micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(real_labels, pred_labels, average='micro')\n",
    "    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(real_labels, pred_labels, average='macro')\n",
    "    \n",
    "\n",
    "    report = classification_report(real_labels, pred_labels)\n",
    "\n",
    "    print('f1 micro: ',f1_micro)\n",
    "    print('f1 macro: ',f1_macro)\n",
    "    print('f1 weighted: ',f1_weighted)\n",
    "    #print('f1 binary: ',f1_binary)\n",
    "    #print('f1 samples: ',f1_samples)\n",
    "    print('micro p, micro r, micro f1:', micro_p, micro_r, micro_f1)\n",
    "    print('macro p, macro r, macro f1:', macro_p, macro_r, macro_f1)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24783, 100), (24783, 70), (24783, 17))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ski = pd.read_csv('../data/word2vec_skip2.csv',header=None)\n",
    "X_lda = pd.read_csv('../data/lda_infer.csv')\n",
    "X_liwc = pd.read_csv('../data/LIWC.csv')\n",
    "y = pd.read_csv('../data/labeled_data.csv')['class']\n",
    "X_ski.shape,X_lda.shape, X_liwc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_ski)\n",
    "X_ski = pd.DataFrame(scaler.transform(X_ski))\n",
    "scaler.fit(X_lda)\n",
    "X_lda = pd.DataFrame(scaler.transform(X_lda))\n",
    "scaler.fit(X_liwc)\n",
    "X_liwc = pd.DataFrame(scaler.transform(X_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.210364</td>\n",
       "      <td>-0.432598</td>\n",
       "      <td>-0.997330</td>\n",
       "      <td>0.870346</td>\n",
       "      <td>-1.034201</td>\n",
       "      <td>0.125785</td>\n",
       "      <td>-0.308096</td>\n",
       "      <td>0.768088</td>\n",
       "      <td>-0.112404</td>\n",
       "      <td>-0.485286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>0.934323</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970142</td>\n",
       "      <td>0.184729</td>\n",
       "      <td>0.762217</td>\n",
       "      <td>0.069552</td>\n",
       "      <td>0.504674</td>\n",
       "      <td>0.281666</td>\n",
       "      <td>-1.103539</td>\n",
       "      <td>-0.847527</td>\n",
       "      <td>0.553633</td>\n",
       "      <td>1.255813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>1.577765</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.507980</td>\n",
       "      <td>-0.775718</td>\n",
       "      <td>0.262399</td>\n",
       "      <td>0.749621</td>\n",
       "      <td>-0.418676</td>\n",
       "      <td>-0.965173</td>\n",
       "      <td>0.799103</td>\n",
       "      <td>0.480935</td>\n",
       "      <td>-0.230887</td>\n",
       "      <td>-0.661697</td>\n",
       "      <td>...</td>\n",
       "      <td>1.818742</td>\n",
       "      <td>3.010036</td>\n",
       "      <td>1.665324</td>\n",
       "      <td>0.934323</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.987364</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.718547</td>\n",
       "      <td>0.118731</td>\n",
       "      <td>-1.381494</td>\n",
       "      <td>-0.043071</td>\n",
       "      <td>0.471597</td>\n",
       "      <td>1.835291</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>0.099431</td>\n",
       "      <td>-1.110124</td>\n",
       "      <td>-0.801691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>1.577765</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>-0.577253</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.889593</td>\n",
       "      <td>-1.040487</td>\n",
       "      <td>1.200376</td>\n",
       "      <td>0.996471</td>\n",
       "      <td>-0.239872</td>\n",
       "      <td>-1.111058</td>\n",
       "      <td>0.795121</td>\n",
       "      <td>0.486174</td>\n",
       "      <td>0.017350</td>\n",
       "      <td>-0.311059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>0.439394</td>\n",
       "      <td>1.183971</td>\n",
       "      <td>-0.918551</td>\n",
       "      <td>-0.949504</td>\n",
       "      <td>0.048243</td>\n",
       "      <td>1.495339</td>\n",
       "      <td>-1.156798</td>\n",
       "      <td>-0.855282</td>\n",
       "      <td>0.710415</td>\n",
       "      <td>1.252661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>-0.577253</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>-0.237918</td>\n",
       "      <td>-1.377332</td>\n",
       "      <td>0.347184</td>\n",
       "      <td>1.690207</td>\n",
       "      <td>-1.265389</td>\n",
       "      <td>-0.788985</td>\n",
       "      <td>0.281229</td>\n",
       "      <td>1.067327</td>\n",
       "      <td>0.934023</td>\n",
       "      <td>0.176924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>-0.727257</td>\n",
       "      <td>-1.709331</td>\n",
       "      <td>1.590391</td>\n",
       "      <td>1.838453</td>\n",
       "      <td>1.139058</td>\n",
       "      <td>-1.406139</td>\n",
       "      <td>-0.940876</td>\n",
       "      <td>0.227202</td>\n",
       "      <td>-1.304643</td>\n",
       "      <td>0.536335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>1.665324</td>\n",
       "      <td>0.934323</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.987364</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>1.241618</td>\n",
       "      <td>0.058139</td>\n",
       "      <td>0.255205</td>\n",
       "      <td>-0.361494</td>\n",
       "      <td>0.767290</td>\n",
       "      <td>-0.042059</td>\n",
       "      <td>0.154172</td>\n",
       "      <td>0.396215</td>\n",
       "      <td>-1.199854</td>\n",
       "      <td>0.053228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556133</td>\n",
       "      <td>1.250531</td>\n",
       "      <td>1.665324</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>-0.439245</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>0.893203</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>-0.089751</td>\n",
       "      <td>1.726396</td>\n",
       "      <td>-1.457368</td>\n",
       "      <td>-1.187706</td>\n",
       "      <td>0.232446</td>\n",
       "      <td>1.920649</td>\n",
       "      <td>-1.809326</td>\n",
       "      <td>-1.150480</td>\n",
       "      <td>0.276188</td>\n",
       "      <td>0.791993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706476</td>\n",
       "      <td>-0.508973</td>\n",
       "      <td>-0.424595</td>\n",
       "      <td>-0.562272</td>\n",
       "      <td>-0.660135</td>\n",
       "      <td>-0.555307</td>\n",
       "      <td>3.594774</td>\n",
       "      <td>-0.460441</td>\n",
       "      <td>-0.577253</td>\n",
       "      <td>-0.49192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.210364 -0.432598 -0.997330  0.870346 -1.034201  0.125785 -0.308096   \n",
       "1     -0.970142  0.184729  0.762217  0.069552  0.504674  0.281666 -1.103539   \n",
       "2     -0.507980 -0.775718  0.262399  0.749621 -0.418676 -0.965173  0.799103   \n",
       "3     -0.718547  0.118731 -1.381494 -0.043071  0.471597  1.835291 -1.708760   \n",
       "4     -0.889593 -1.040487  1.200376  0.996471 -0.239872 -1.111058  0.795121   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24778  0.439394  1.183971 -0.918551 -0.949504  0.048243  1.495339 -1.156798   \n",
       "24779 -0.237918 -1.377332  0.347184  1.690207 -1.265389 -0.788985  0.281229   \n",
       "24780 -0.727257 -1.709331  1.590391  1.838453  1.139058 -1.406139 -0.940876   \n",
       "24781  1.241618  0.058139  0.255205 -0.361494  0.767290 -0.042059  0.154172   \n",
       "24782 -0.089751  1.726396 -1.457368 -1.187706  0.232446  1.920649 -1.809326   \n",
       "\n",
       "             7         8         9   ...        7         8         9   \\\n",
       "0      0.768088 -0.112404 -0.485286  ...  0.556133  1.250531 -0.424595   \n",
       "1     -0.847527  0.553633  1.255813  ...  0.556133  1.250531 -0.424595   \n",
       "2      0.480935 -0.230887 -0.661697  ...  1.818742  3.010036  1.665324   \n",
       "3      0.099431 -1.110124 -0.801691  ... -0.706476 -0.508973 -0.424595   \n",
       "4      0.486174  0.017350 -0.311059  ...  0.556133  1.250531 -0.424595   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24778 -0.855282  0.710415  1.252661  ... -0.706476 -0.508973 -0.424595   \n",
       "24779  1.067327  0.934023  0.176924  ... -0.706476 -0.508973 -0.424595   \n",
       "24780  0.227202 -1.304643  0.536335  ...  0.556133  1.250531  1.665324   \n",
       "24781  0.396215 -1.199854  0.053228  ...  0.556133  1.250531  1.665324   \n",
       "24782 -1.150480  0.276188  0.791993  ... -0.706476 -0.508973 -0.424595   \n",
       "\n",
       "             10        11        12        13        14        15       16  \n",
       "0      0.934323 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "1     -0.562272 -0.660135 -0.555307  1.577765 -0.460441  0.893203 -0.49192  \n",
       "2      0.934323  0.622409  0.987364 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "3     -0.562272 -0.660135 -0.555307  1.577765 -0.460441 -0.577253 -0.49192  \n",
       "4     -0.562272 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "...         ...       ...       ...       ...       ...       ...      ...  \n",
       "24778 -0.562272 -0.660135 -0.555307 -0.439245 -0.460441 -0.577253 -0.49192  \n",
       "24779 -0.562272 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "24780  0.934323  0.622409  0.987364 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "24781 -0.562272 -0.660135 -0.555307 -0.439245 -0.460441  0.893203 -0.49192  \n",
       "24782 -0.562272 -0.660135 -0.555307  3.594774 -0.460441 -0.577253 -0.49192  \n",
       "\n",
       "[24783 rows x 187 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda.columns = list(range(101,171))\n",
    "X_all = pd.concat([X_ski, X_lda, X_liwc], axis=1)\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train, X_all_test, y_train, y_test = train_test_split(X_all, y, random_state=17, test_size=0.1)\n",
    "X_ski_train, X_ski_test = X_all_train.iloc[:, list(range(0,100))], X_all_test.iloc[:, list(range(0,100))]\n",
    "X_lda_train, X_lda_test = X_all_train.iloc[:, list(range(100,170))], X_all_test.iloc[:, list(range(100,170))]\n",
    "X_liwc_train, X_liwc_test = X_all_train.iloc[:, list(range(170,187))], X_all_test.iloc[:, list(range(170,187))]\n",
    "X_liwc_lda_train, X_liwc_lda_test = pd.concat([X_lda_train, X_liwc_train], axis=1), pd.concat([X_lda_test, X_liwc_test], axis=1)\n",
    "X_ski_lda_train, X_ski_lda_test = pd.concat([X_lda_train, X_ski_train], axis=1), pd.concat([X_lda_test, X_ski_test], axis=1)\n",
    "X_liwc_ski_train, X_liwc_ski_test = pd.concat([X_ski_train, X_liwc_train], axis=1), pd.concat([X_ski_test, X_liwc_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8846308995562727\n",
      "f1 macro:  0.6409930966529657\n",
      "f1 weighted:  0.8991726366801739\n",
      "micro p, micro r, micro f1: 0.8846308995562727 0.8846308995562727 0.8846308995562727\n",
      "macro p, macro r, macro f1: 0.6353561160087069 0.7208947581600446 0.6409930966529657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.49      0.19        35\n",
      "           1       0.95      0.92      0.93      2010\n",
      "           2       0.84      0.76      0.79       434\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.64      0.72      0.64      2479\n",
      "weighted avg       0.92      0.88      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_all_train, y_train)\n",
    "y_preds = model.predict(X_all_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8830173457039129\n",
      "f1 macro:  0.6390490485413304\n",
      "f1 weighted:  0.8980547360428501\n",
      "micro p, micro r, micro f1: 0.8830173457039129 0.8830173457039129 0.8830173457039129\n",
      "macro p, macro r, macro f1: 0.6312973771712175 0.7387798107721718 0.6390490485413304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.55      0.20        31\n",
      "           1       0.95      0.92      0.93      2017\n",
      "           2       0.82      0.75      0.79       431\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.63      0.74      0.64      2479\n",
      "weighted avg       0.92      0.88      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_ski_train, y_train)\n",
    "y_preds = model.predict(X_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.7918515530455829\n",
      "f1 macro:  0.37432030499552615\n",
      "f1 weighted:  0.8528944220732593\n",
      "micro p, micro r, micro f1: 0.7918515530455829 0.7918515530455829 0.7918515530455829\n",
      "macro p, macro r, macro f1: 0.3775714552912366 0.44655208240567584 0.37432030499552615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.98      0.80      0.88      2364\n",
      "           2       0.15      0.54      0.24       114\n",
      "\n",
      "    accuracy                           0.79      2479\n",
      "   macro avg       0.38      0.45      0.37      2479\n",
      "weighted avg       0.94      0.79      0.85      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_lda_train, y_train)\n",
    "y_preds = model.predict(X_lda_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.7841871722468737\n",
      "f1 macro:  0.29301379154420076\n",
      "f1 weighted:  0.8786867794834967\n",
      "micro p, micro r, micro f1: 0.7841871722468737 0.7841871722468737 0.7841871722468737\n",
      "macro p, macro r, macro f1: 0.3331619537275064 0.26150121065375304 0.29301379154420076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.78      0.88      2478\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.78      2479\n",
      "   macro avg       0.33      0.26      0.29      2479\n",
      "weighted avg       1.00      0.78      0.88      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_train, y_train)\n",
    "y_preds = model.predict(X_liwc_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.7958854376764825\n",
      "f1 macro:  0.40876158511903316\n",
      "f1 weighted:  0.8436507391061131\n",
      "micro p, micro r, micro f1: 0.7958854376764825 0.7958854376764825 0.7958854376764825\n",
      "macro p, macro r, macro f1: 0.40424708241445156 0.45470788806052465 0.40876158511903316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.96      0.82      0.88      2296\n",
      "           2       0.25      0.55      0.34       179\n",
      "\n",
      "    accuracy                           0.80      2479\n",
      "   macro avg       0.40      0.45      0.41      2479\n",
      "weighted avg       0.91      0.80      0.84      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_lda_train, y_train)\n",
    "y_preds = model.predict(X_liwc_lda_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8838241226300928\n",
      "f1 macro:  0.6360190315776292\n",
      "f1 weighted:  0.8989315314304018\n",
      "micro p, micro r, micro f1: 0.8838241226300928 0.8838241226300928 0.8838241226300928\n",
      "macro p, macro r, macro f1: 0.6294305636077457 0.7159807019725203 0.6360190315776292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.47      0.18        34\n",
      "           1       0.95      0.92      0.93      2019\n",
      "           2       0.82      0.76      0.79       426\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.63      0.72      0.64      2479\n",
      "weighted avg       0.92      0.88      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_ski_lda_train, y_train)\n",
    "y_preds = model.predict(X_ski_lda_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8850342880193626\n",
      "f1 macro:  0.6454233200252131\n",
      "f1 weighted:  0.8993412883818996\n",
      "micro p, micro r, micro f1: 0.8850342880193627 0.8850342880193627 0.8850342880193626\n",
      "macro p, macro r, macro f1: 0.6404356447210048 0.7433920838534395 0.6454233200252131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.56      0.21        32\n",
      "           1       0.95      0.92      0.93      2001\n",
      "           2       0.85      0.75      0.79       446\n",
      "\n",
      "    accuracy                           0.89      2479\n",
      "   macro avg       0.64      0.74      0.65      2479\n",
      "weighted avg       0.92      0.89      0.90      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_ski_train, y_train)\n",
    "y_preds = model.predict(X_liwc_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems LIWC & Skip-Learn seems to be the best combination. \n",
    "Since the Coherence Score on LDA is lower than 0.5, \n",
    "it could be a good idea to remove lda features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22304, 61)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',\n",
    "                                            penalty=\"l1\",C=0.01, solver = 'saga', \n",
    "                                            max_iter = 2000)).fit(X_liwc_ski_train,y_train)\n",
    "#X_liwc_ski_train_ = select.fit_transform(X_liwc_ski_train,y_train)\n",
    "\n",
    "\n",
    "feature_idx = select.get_support()\n",
    "selected_columns = X_liwc_ski_train.columns[feature_idx]\n",
    "X_liwc_ski_train_ = X_liwc_ski_train[selected_columns]\n",
    "X_liwc_ski_test_ = X_liwc_ski_test[selected_columns]\n",
    "X_liwc_ski_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22304, 84)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = X_liwc_ski_train.corr()\n",
    "\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.3:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = X_liwc_ski_train.columns[columns]\n",
    "X_liwc_ski_train_f = X_liwc_ski_train[selected_columns]\n",
    "X_liwc_ski_test_f = X_liwc_ski_test[selected_columns]\n",
    "X_train_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8781766841468334\n",
      "f1 macro:  0.6231050690803706\n",
      "f1 weighted:  0.8938602955502181\n",
      "micro p, micro r, micro f1: 0.8781766841468334 0.8781766841468334 0.8781766841468334\n",
      "macro p, macro r, macro f1: 0.6226121035759181 0.7201859052757533 0.6231050690803706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.52      0.17        27\n",
      "           1       0.95      0.92      0.93      2005\n",
      "           2       0.82      0.72      0.77       447\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.62      0.72      0.62      2479\n",
      "weighted avg       0.91      0.88      0.89      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter = 2000).fit(X_liwc_ski_train_, y_train)\n",
    "y_preds = model.predict(X_liwc_ski_test_)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Model has better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro:  0.8842275110931828\n",
      "f1 macro:  0.6105923159112313\n",
      "f1 weighted:  0.9035868503861484\n",
      "micro p, micro r, micro f1: 0.8842275110931828 0.8842275110931828 0.8842275110931828\n",
      "macro p, macro r, macro f1: 0.6173225136752598 0.7136861722478759 0.6105923159112313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.47      0.10        17\n",
      "           1       0.95      0.92      0.93      2022\n",
      "           2       0.84      0.75      0.80       440\n",
      "\n",
      "    accuracy                           0.88      2479\n",
      "   macro avg       0.62      0.71      0.61      2479\n",
      "weighted avg       0.93      0.88      0.90      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "model = svm.LinearSVC(max_iter = 2000).fit(X_liwc_ski_train, y_train)\n",
    "y_preds = model.predict(X_liwc_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression has better performance, and SVM doesn't converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, \n",
    "                           cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_liwc_ski_train, y_train)\n",
    "y_preds = grid_result.predict(X_liwc_ski_test)\n",
    "evaluation(y_preds, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
