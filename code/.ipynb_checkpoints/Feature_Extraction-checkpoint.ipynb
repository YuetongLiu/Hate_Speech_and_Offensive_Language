{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to GitHub:\n",
    "\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import csv\n",
    "from liwc import LIWC\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  hate_speech  offensive_language  neither  class  \\\n",
      "0      3            0                   0        3      2   \n",
      "1      3            0                   3        0      1   \n",
      "2      3            0                   3        0      1   \n",
      "3      3            0                   2        1      1   \n",
      "4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "(24783, 6)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/labeled_data.csv\")\n",
    "df.drop(df.columns[[0]], axis=1,inplace=True)\n",
    "print(df.iloc[:5,:])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     mayasolovely woman shouldn complain cleaning ...\n",
      "1     mleew boy das coldyga dwn bad cuffin da hoe s...\n",
      "2     ukindofband dawg  sbabylife eve fuck bich sa ...\n",
      "3                  cgandeson vivabased look like anny \n",
      "4     shenikaobes shi hea migh ue migh fake bich ol...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "txt=df.iloc[:,5]\n",
    "tweet=txt.copy(deep=True)\n",
    "#tweet=pd.DataFrame(tweet)\n",
    "#transform to lower case\n",
    "tweet=tweet.str.lower()\n",
    "#remove punctuation\n",
    "remove = str.maketrans('','',string.punctuation) \n",
    "tweet = tweet.str.translate(remove)\n",
    "#word tokenize and remove stopwords\n",
    "#remove digit and excessive whitespace\n",
    "for i in range(tweet.shape[0]):\n",
    "    text=tweet[i]\n",
    "    text1=''.join([ch+\" \" for ch in text.split() if ch not in ' 123456789'])\n",
    "    text2=''.join([word+\" \" for word in text1.split() if word not in stopwords])\n",
    "    tweet[i]=text2\n",
    "info=re.compile('[0-9|rt]')\n",
    "tweet=tweet.apply(lambda x: info.sub('',x))\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do more process of replacing:\n",
    "    1) urls\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     mayasolovely woman shouldn complain cleaning ...\n",
      "1     mleew boy das coldyga dwn bad cuffin da hoe s...\n",
      "2     ukindofband dawg sbabylife eve fuck bich sa c...\n",
      "3                  cgandeson vivabased look like anny \n",
      "4     shenikaobes shi hea migh ue migh fake bich ol...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text_string):\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "for i in range(tweet.shape[0]):\n",
    "    text=tweet[i]\n",
    "    text1=preprocess(text)\n",
    "    tweet[i]=text1\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "## 1.LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LIWC on the whole df\n",
    "\n",
    "LIWC_list=[]\n",
    "for i in range(len(tweet)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(tweet[i].split())\n",
    "    LIWC_list.append(L.readable(labels))\n",
    "LIWC_list\n",
    "#update preprocessed text to original df\n",
    "df.tweet=tweet\n",
    "hateSpeech=df[df['hate_speech'] ==3][\"tweet\"].reset_index(drop=True)\n",
    "#it is the list of hate speech\n",
    "LIWC_list1=[]\n",
    "for i in range(len(hateSpeech)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(hateSpeech[i].split())\n",
    "    LIWC_list1.append(L.readable(labels))\n",
    "LIWC_list1\n",
    "#function that add dictionary\n",
    "def mergeDict(dict1, dict2):\n",
    "   ''' Merge dictionaries and keep values of common keys in list'''\n",
    "   dict3 = {**dict1, **dict2}\n",
    "   for key, value in dict3.items():\n",
    "       if key in dict1 and key in dict2:\n",
    "               dict3[key] = value +dict1[key]\n",
    "   return dict3\n",
    "#add those dic together\n",
    "hate_dic=LIWC_list1[0]\n",
    "for i in range(len(LIWC_list1)):\n",
    "    hate_dic=mergeDict(hate_dic, LIWC_list1[i])\n",
    "hate_dic.keys()\n",
    "#find hate_relevent key by analyzing hate speech\n",
    "relevent_key=[k for k, v in hate_dic.items() if v > 50]\n",
    "#build dataframe with relevent columns\n",
    "LIWC_df=pd.DataFrame(data=LIWC_list, index=None, columns=None, dtype=None, copy=False)[relevent_key]\n",
    "LIWC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_df = LIWC_df.fillna(0)\n",
    "LIWC_df.to_csv(\"../data/LIWC.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Skip-gram\n",
    "\n",
    "### Average len of tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.66953153371263\n"
     ]
    }
   ],
   "source": [
    "#average len of tweet\n",
    "avg=0\n",
    "num=[]\n",
    "x=[x for x in range(24783)]\n",
    "for n in range(tweet.shape[0]):\n",
    "    avg+=len(tweet[n])\n",
    "    num.append(len(tweet[n]))\n",
    "avg=avg/(tweet.shape[0])\n",
    "plt.bar(x,num)\n",
    "plt.show()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212672"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=[]\n",
    "for i in range(tweet.shape[0]):\n",
    "    w=tweet[i].replace('\\n','').rstrip()\n",
    "    w1=w.split(\" \")\n",
    "    word.append(w1)\n",
    "def flatten(seq):\n",
    "    s=str(seq).replace('[', '').replace(']', '') \n",
    "    return [eval(x) for x in s.split(',') if x.strip()] \n",
    "word=[x for x in flatten(word)]\n",
    "word=[x for x in word if x!='']\n",
    "len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the input data for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [mayasolovely, woman, shouldn, complain, clean...\n",
       "1    [mleew, boy, das, coldyga, dwn, bad, cuffin, d...\n",
       "2    [ukindofband, dawg, sbabylife, eve, fuck, bich...\n",
       "3             [cgandeson, vivabased, look, like, anny]\n",
       "4    [shenikaobes, shi, hea, migh, ue, migh, fake, ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tweet)):\n",
    "    doc=tweet[i]\n",
    "    doc=doc.split(\" \")\n",
    "    doc=[x for x in doc if x!='']\n",
    "    tweet[i]=doc\n",
    "tweet[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec by skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721048, 1063360)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "model = Word2Vec(size=100, workers=5,sg=1)  #dimension is 200, consider 5 words, using skip-gram\n",
    "model.build_vocab(tweet)\n",
    "model.train(tweet,total_examples = model.corpus_count,epochs = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-900bfdd4be2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mword_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "word_vec=[]\n",
    "#average each word vector to get the vector of a single tweet\n",
    "for i in range(len(tweet)):\n",
    "    sum1=0\n",
    "    for w in tweet[i]:\n",
    "        try:\n",
    "            sum1+=model[w]\n",
    "        except:\n",
    "            continue\n",
    "    avg=sum1/(len(tweet[i]))\n",
    "    avg=list(avg)\n",
    "    word_vec.append(avg)\n",
    "word_vec[:5]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 598 is out of bounds for axis 0 with size 598",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-0b609e3b24e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword_vec_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0marray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_vec_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_vec_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mword_vec_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 598 is out of bounds for axis 0 with size 598"
     ]
    }
   ],
   "source": [
    "word_vec_array=np.array(word_vec)\n",
    "word_vec_l=np.reshape(word_vec_array[0],(1,100))\n",
    "for i in range(len(tweet)):\n",
    "    array=np.reshape(word_vec_array[i],(1,100))\n",
    "    word_vec_l=np.r_[word_vec_l,array]\n",
    "word_vec_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_lt=word_vec_l\n",
    "word_vec_lt=np.delete(word_vec_lt,0,0)\n",
    "word_vec_lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save this array as csv\n",
    "\n",
    "f = open('../data/word2vec_skip.csv', 'w')\n",
    "a = csv.writer(f)\n",
    "a.writerows(word_vec_lt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('../data/word2vec_skip1.csv', word_vec_lt, delimiter = ',')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.LDA\n",
    "\n",
    "construct dic for LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tweet)\n",
    "corpus = [dictionary.doc2bow(words) for words in tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.38819085916417634\n",
      "\n",
      "Coherence Score:  0.42194391540606735\n",
      "\n",
      "Coherence Score:  0.43920979717599234\n",
      "\n",
      "Coherence Score:  0.41266646537218143\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 60, 70, 80]:\n",
    "    lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=i)\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=tweet, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, '0.200*\"man\" + 0.074*\"show\" + 0.043*\"eveyhing\" + 0.039*\"bich\" + 0.028*\"one\"')\n",
      "(68, '0.147*\"n\" + 0.121*\"us\" + 0.066*\"wo\" + 0.047*\"done\" + 0.043*\"bae\"')\n",
      "(55, '0.165*\"alk\" + 0.130*\"bou\" + 0.033*\"kid\" + 0.029*\"youve\" + 0.027*\"bich\"')\n",
      "(30, '0.122*\"bee\" + 0.108*\"yellow\" + 0.067*\"bich\" + 0.056*\"ied\" + 0.043*\"ask\"')\n",
      "(23, '0.134*\"fa\" + 0.128*\"new\" + 0.077*\"bich\" + 0.063*\"die\" + 0.046*\"ype\"')\n",
      "(13, '0.108*\"geing\" + 0.088*\"black\" + 0.072*\"nohing\" + 0.057*\"h\" + 0.051*\"kids\"')\n",
      "(67, '0.141*\"gil\" + 0.103*\"call\" + 0.087*\"wie\" + 0.044*\"like\" + 0.042*\"days\"')\n",
      "(10, '0.087*\"sa\" + 0.063*\"song\" + 0.061*\"had\" + 0.047*\"sick\" + 0.047*\"pussies\"')\n",
      "(37, '0.118*\"gonna\" + 0.056*\"fee\" + 0.043*\"im\" + 0.043*\"use\" + 0.037*\"somebody\"')\n",
      "(3, '0.113*\"lile\" + 0.110*\"bich\" + 0.096*\"always\" + 0.043*\"wish\" + 0.043*\"hai\"')\n",
      "(39, '0.077*\"hi\" + 0.068*\"niggah\" + 0.064*\"gon\" + 0.064*\"onigh\" + 0.062*\"back\"')\n",
      "(11, '0.107*\"alking\" + 0.079*\"well\" + 0.074*\"side\" + 0.041*\"ges\" + 0.037*\"calling\"')\n",
      "(8, '0.146*\"look\" + 0.116*\"like\" + 0.085*\"bich\" + 0.077*\"sop\" + 0.070*\"sill\"')\n",
      "(0, '0.157*\"cun\" + 0.047*\"away\" + 0.045*\"acing\" + 0.043*\"maybe\" + 0.037*\"jihadi\"')\n",
      "(31, '0.104*\"mad\" + 0.059*\"find\" + 0.041*\"body\" + 0.036*\"slow\" + 0.033*\"ick\"')\n",
      "(42, '0.077*\"dumb\" + 0.072*\"yea\" + 0.056*\"women\" + 0.055*\"ima\" + 0.052*\"ah\"')\n",
      "(2, '0.098*\"ca\" + 0.082*\"anohe\" + 0.069*\"looks\" + 0.066*\"like\" + 0.052*\"bich\"')\n",
      "(54, '0.233*\"know\" + 0.067*\"bich\" + 0.045*\"boke\" + 0.040*\"hings\" + 0.034*\"biches\"')\n",
      "(25, '0.090*\"house\" + 0.082*\"hope\" + 0.051*\"bich\" + 0.047*\"wife\" + 0.042*\"pop\"')\n",
      "(48, '0.094*\"bes\" + 0.085*\"soy\" + 0.079*\"wach\" + 0.067*\"hell\" + 0.065*\"young\"')\n",
      "(array([[0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 1.010879  , 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       ...,\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571]], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# lda model, num_topics is the number of topic\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=70)\n",
    "# print 5 words for each topic \n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic)\n",
    "# topic infer\n",
    "lda_infer= lda.inference(corpus)\n",
    "print(lda.inference(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 70)\n"
     ]
    }
   ],
   "source": [
    "print(lda_infer[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_infer = pd.DataFrame(lda_infer[0])\n",
    "lda_infer.to_csv(\"../data/lda_infer.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
