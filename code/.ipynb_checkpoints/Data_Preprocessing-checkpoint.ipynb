{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to GitHub: \n",
    "    \n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  hate_speech  offensive_language  neither  class  \\\n",
      "0      3            0                   0        3      2   \n",
      "1      3            0                   3        0      1   \n",
      "2      3            0                   3        0      1   \n",
      "3      3            0                   2        1      1   \n",
      "4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "(24783, 6)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/labeled_data.csv\")\n",
    "df.drop(df.columns[[0]], axis=1,inplace=True)\n",
    "print(df.iloc[:5,:])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     mayasolovely woman shouldn complain cleaning ...\n",
      "1     mleew boy das coldyga dwn bad cuffin da hoe s...\n",
      "2     ukindofband dawg  sbabylife eve fuck bich sa ...\n",
      "3                  cgandeson vivabased look like anny \n",
      "4     shenikaobes shi hea migh ue migh fake bich ol...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "txt=df.iloc[:,5]\n",
    "tweet=txt.copy(deep=True)\n",
    "#tweet=pd.DataFrame(tweet)\n",
    "#transform to lower case\n",
    "tweet=tweet.str.lower()\n",
    "#remove punctuation\n",
    "remove = str.maketrans('','',string.punctuation) \n",
    "tweet = tweet.str.translate(remove)\n",
    "#word tokenize and remove stopwords\n",
    "#remove digit and excessive whitespace\n",
    "for i in range(tweet.shape[0]):\n",
    "    text=tweet[i]\n",
    "    text1=''.join([ch+\" \" for ch in text.split() if ch not in ' 123456789'])\n",
    "    text2=''.join([word+\" \" for word in text1.split() if word not in stopwords])\n",
    "    tweet[i]=text2\n",
    "info=re.compile('[0-9|rt]')\n",
    "tweet=tweet.apply(lambda x: info.sub('',x))\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do more process of replacing:\n",
    "    1) urls\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     mayasolovely woman shouldn complain cleaning ...\n",
      "1     mleew boy das coldyga dwn bad cuffin da hoe s...\n",
      "2     ukindofband dawg sbabylife eve fuck bich sa c...\n",
      "3                  cgandeson vivabased look like anny \n",
      "4     shenikaobes shi hea migh ue migh fake bich ol...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text_string):\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "for i in range(tweet.shape[0]):\n",
    "    text=tweet[i]\n",
    "    text1=preprocess(text)\n",
    "    tweet[i]=text1\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.DataFrame(tweet)\n",
    "np.savetxt('../data/cleaned_X.csv', tweet, format('%s'))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
