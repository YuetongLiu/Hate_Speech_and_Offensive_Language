{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9VDfC-ie19P"
   },
   "source": [
    "# Hate_Speech_and_Offensive_Language\n",
    "\n",
    "Project mentor: Carlos Aguirre\n",
    "\n",
    "Yuxiang Wang <ywang594@jh.edu>, Jingxi Liu <jliu238@jh.edu>, Wenkai Luo <wluo14@jh.edu>, Yuetong Liu<yliu390@jh.edu>\n",
    "\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqwI3PT-hBJo"
   },
   "source": [
    "# Outline and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7Af6y48e7HI"
   },
   "source": [
    "### Uncompleted Deliverables\n",
    "1. \"Would like to complete #2\": Unsupervised learning: clustering hate speech to identify major topics, including race, colour, sex (ran out of time).\n",
    "2. \"Would like to complete #3\": Improvement: link data with CF users to study who are more likely to use hate speech/offensive language. Integrate the frequency of a user detected for posting hate speech and the user’s interaction with others (ran out of time).\n",
    "\n",
    "\n",
    "### Completed Deliverables\n",
    "1. \"Must complete #1\": Data Pre-processing: pre-process text data including deleting duplicates, removing stop words or punctuation, and convertingtweet content to lowercase. in \"Pre-processing\" below.\n",
    "2. \"Must complete #2\": Feature Extraction: Combine skip-gram, LIWC and LDA methods to extract features. in \"Pre-processing\" below.\n",
    "3. \"Must complete #3\": Feature Selection: Use PMI on the training data to quantified the importance of each feature. Set up a threshold to choose themost beneficial features. Most features share similar mutual information, so we switched Logistics Regression with L1 regulization.[in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb).\n",
    "4. \"Expect to complete #1\": EDA: conduct exploratory data analysis. Moreover, conduct correlation analysis between features.in \"Pre-processing\" below and [in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb)\n",
    "5. \"Expect to complete #2\": Prediction: Use the F1 value on the Davidson, T. et al. (2017) as a baseline, accurately classify the text comment into hate speech,offensive language or neither. Otherwise, interpret the model to explain the mis-classification.[in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb)\n",
    "6. \"Expect to complete #3\": Model Selection: Besides logistic regression model, use MLP + CNN as a comparisons to train data and make model selectionbased on their performance. Use cross validation to find optimal hyper-parameters. in \"Deep Learning Methods\" below]\n",
    "7. \"Would like to complete #1\": Beside improving the precision, choose model based onevaluation metrics. [in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb)\n",
    "\n",
    "\n",
    "### Additional Deliverables\n",
    "1. We add TFIDF as a new method to extract features in \"Pre-processing\" below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eiq2aSauhSsS"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtWkhiIPfOfK"
   },
   "source": [
    "## What problem were you trying to solve or understand?\n",
    "\n",
    "What are the real-world implications of this data and task?\n",
    "\n",
    "How is this problem similar to others we’ve seen in lectures, breakouts, and homeworks?\n",
    "\n",
    "What makes this problem unique?\n",
    "\n",
    "What ethical implications does this problem have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFq-_D0khnhh"
   },
   "source": [
    "## Dataset(s)\n",
    "\n",
    "Describe the dataset(s) you used.\n",
    "\n",
    "How were they collected?\n",
    "\n",
    "Why did you choose them?\n",
    "\n",
    "How many examples in each?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lOicoBYif7g"
   },
   "outputs": [],
   "source": [
    "# Load your data and print 2-3 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XN1fYEfGidiD"
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "What features did you use or choose not to use? Why?\n",
    "\n",
    "If you have categorical labels, were your datasets class-balanced?\n",
    "\n",
    "How did you deal with missing data? What about outliers?\n",
    "\n",
    "What approach(es) did you use to pre-process your data? Why?\n",
    "\n",
    "Are your features continuous or categorical? How do you treat these features differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEuKEzM5ipag"
   },
   "outputs": [],
   "source": [
    "# For those same examples above, what do they look like after being pre-processed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cDLEwhAx0gP"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of your data before and after pre-processing.\n",
    "#   You may borrow from how we visualized data in the Lab homeworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tASjmmtjiwvu"
   },
   "source": [
    "# Models and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlrwR9E1hnQ3"
   },
   "source": [
    "## Experimental Setup\n",
    "\n",
    "How did you evaluate your methods? Why is that a reasonable evaluation metric for the task?\n",
    "\n",
    "What did you use for your loss function to train your models? Did you try multiple loss functions? Why or why not?\n",
    "\n",
    "How did you split your data into train and test sets? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUNxC358jPDr"
   },
   "source": [
    "### Code for loss functions, evaluation metrics or link to Git repo:\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMyqHUa0jUw7"
   },
   "source": [
    "## Baselines \n",
    "\n",
    "What baselines did you compare against? Why are these reasonable?\n",
    "\n",
    "Did you look at related work to contextualize how others methods or baselines have performed on this dataset/task? If so, how did those methods do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqB48IF9kMBf"
   },
   "source": [
    "## Classic Methods\n",
    "\n",
    "### What methods did you choose? Why did you choose them?\n",
    "Based on the prior work and literature review. Both logistic regression and SVM seem promising methods for this classifying application.In this project, we will be focusing on logistic regression with L2 regularization. A one-versus-rest framework will be applied in thisproject where each classifier will be trained separately for the specific class. During the testing stage, the class with the highest predictedprobability will be labelled for the corresponding sample.\n",
    "\n",
    "### How did you train these methods, and how did you evaluate them? Why?\n",
    "We trained this models and evaluated them based on F1_mirco_score. We use micro score because the data is inbalanced, and we are more interested in performance of the minor group(hate speech)\n",
    "\n",
    "### Which methods were easy/difficult to implement and train? Why?\n",
    "The model is easy to implement. However, it takes long time to train due to the large dimension of the sparse features.\n",
    "\n",
    "### For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?\n",
    "For logistic Regression, we tuned regularization strength C and tried several optimization algorithms. The performance doesn't have very significant change after hyperparameter tuning, but the accuracy does increase.\n",
    "\n",
    "### Code for training models, or link to your Git repository:\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqB48IF9kMBf"
   },
   "source": [
    "## Deep Learning Methods\n",
    "\n",
    "What methods did you choose? Why did you choose them?\n",
    "\n",
    "How did you train these methods, and how did you evaluate them? Why?\n",
    "\n",
    "Which methods were easy/difficult to implement and train? Why?\n",
    "\n",
    "For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training models, or link to your Git repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO_kP1fmkWWk"
   },
   "outputs": [],
   "source": [
    "# Show plots of how these models performed during training.\n",
    "#  For example, plot train loss and train accuracy (or other evaluation metric) on the y-axis,\n",
    "#  with number of iterations or number of examples on the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zdp4_H-kx8H"
   },
   "source": [
    "## Results\n",
    "\n",
    "Show tables comparing your methods to the baselines.\n",
    "\n",
    "What about these results surprised you? Why?\n",
    "\n",
    "Did your models over- or under-fit? How can you tell? What did you do to address these issues?\n",
    "\n",
    "What does the evaluation of your trained models tell you about your data? How do you expect these models might behave differently on different data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS2sjfbglG_V"
   },
   "outputs": [],
   "source": [
    "# Show plots or visualizations of your evaluation metric(s) on the train and test sets.\n",
    "#   What do these plots show about over- or under-fitting?\n",
    "#   You may borrow from how we visualized results in the Lab homeworks.\n",
    "#   Are there aspects of your results that are difficult to visualize? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59EbS1GilSQ_"
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugJXhZKNlUT4"
   },
   "source": [
    "## What you've learned\n",
    "\n",
    "We used lots of concepts that we learned from class. Lectures on SVM and logistic regression are extremely helpful when we are dealing with this classification problem. Moreover, the homework on kernel functions gave us inspriation on extracting numrical features for natural language. Also, the CNN homework helped us to be familiar with the structure of CNN, so than make it easier when we building TextCNN.\n",
    "\n",
    "Since many people have worked on this proejct before, we have many resource for literature review. It saved us many time to figure out the pipeline of this project, and give us some new aspects when approaching the problem. I think literature review would be important for us when working for other projects.\n",
    "\n",
    "After the presentation, other groups gave us some suggestios on data pre-processing, including remove non-essential words. After removing more noising words from our dataset, we are able to extract features which are more relevant to the topic.\n",
    "\n",
    "Even though we tried several feature extraction methods, not all of them have good performance when fitting into the model. If we could have more time to work on this project, we would want to evaluate the relevance between features and our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project Submission Template",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
