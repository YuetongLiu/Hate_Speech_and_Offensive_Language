{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "from random import choice\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import csv\n",
    "from liwc import LIWC\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  hate_speech  offensive_language  neither  class  \\\n",
      "0      3            0                   0        3      2   \n",
      "1      3            0                   3        0      1   \n",
      "2      3            0                   3        0      1   \n",
      "3      3            0                   2        1      1   \n",
      "4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "(24783, 6)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/labeled_data.csv\")\n",
    "df.drop(df.columns[[0]], axis=1,inplace=True)\n",
    "print(df.iloc[:5,:])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1     !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2     !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3     !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4     !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "5     !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
       "6     !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
       "7     !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
       "8     \" &amp; you might not get ya bitch back &amp; ...\n",
       "9     \" @rhythmixx_ :hobbies include: fighting Maria...\n",
       "10    \" Keeks is a bitch she curves everyone \" lol I...\n",
       "11                   \" Murda Gang bitch its Gang Land \"\n",
       "12    \" So hoes that smoke are losers ? \" yea ... go...\n",
       "13        \" bad bitches is the only thing that i like \"\n",
       "14                              \" bitch get up off me \"\n",
       "15                      \" bitch nigga miss me with it \"\n",
       "16                               \" bitch plz whatever \"\n",
       "17                            \" bitch who do you love \"\n",
       "18                   \" bitches get cut off everyday B \"\n",
       "19                   \" black bottle &amp; a bad bitch \"\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Yankees\n"
     ]
    }
   ],
   "source": [
    "# Index of tweet only contain format like\"#XX #XX\" (which would be empty after remove hashtag)\n",
    "# We need to specially deal with them\n",
    "special_index=[804,826,846,848,849,923,1016,1122,1909,3398,4818,5711,6098,6279,6332,6668,7168,11951,15859,18062]\n",
    "# for example\n",
    "print(df['tweet'][804])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TNKidsFoodPorn @Oreo ..............&#128530;\n",
      "@hoes &#9829;\n"
     ]
    }
   ],
   "source": [
    "# Also there are two meaningless tweet\n",
    "print(df['tweet'][4828])\n",
    "print(df['tweet'][6098])\n",
    "meaningless_index=[4828,6098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy original text\n",
    "txt=df.iloc[:,4:6]\n",
    "tweet=txt.iloc[:,1].copy(deep=True)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet_,specail_index,meaningless_index):\n",
    "    #remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    other = [\"#ff\", \"ff\", \"rt\"]\n",
    "    stopwords.extend(other)\n",
    "    #word tokenize and remove stopwords\n",
    "    \n",
    "    for i in range(tweet_.shape[0]):\n",
    "        text=tweet_[i].lower()\n",
    "        text1=''.join([word+\" \" for word in text.split() if word not in stopwords])\n",
    "        tweet_[i]=text1\n",
    "    \n",
    "    #remove digit and excessive whitespace\n",
    "    #remove url mention and hashtag\n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    for i in range(tweet_.shape[0]):\n",
    "        text_string=tweet_[i]\n",
    "        parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "        parsed_text = re.sub(mention_regex, '', parsed_text) \n",
    "        if i not in specail_index:\n",
    "            parsed_text = re.sub(hashtag_regex, '', parsed_text)\n",
    "        else:\n",
    "            parsed_text = re.sub('#','',parsed_text)\n",
    "        if i in meaningless_index:\n",
    "            parsed_text ='This is a simple tweet'\n",
    "        tweet_[i]=parsed_text\n",
    "    \n",
    "    #tokenize and stemming\n",
    "    \"\"\"Removes punctuation and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    for i in range(tweet_.shape[0]):\n",
    "        \n",
    "        tweet_stem=tweet_[i]\n",
    "        \n",
    "        remove = str.maketrans('','',string.punctuation) \n",
    "        tweet_stem = tweet_stem.translate(remove).strip()\n",
    "\n",
    "        tweet_process = [stemmer.stem(t) for t in tweet_stem.split()]\n",
    "        \n",
    "        #tweet_[i]=tweet_process\n",
    "        \n",
    "        tweet_[i] = ' '.join([str(elem) for elem in tweet_process]) \n",
    "        \n",
    "\n",
    "        \n",
    "    return tweet_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     woman complain clean hous amp man alway take t...\n",
      "1     boy dat coldtyga dwn bad cuffin dat hoe 1st place\n",
      "2            dawg ever fuck bitch start cri confus shit\n",
      "3                                      look like tranni\n",
      "4        shit hear might true might faker bitch told ya\n",
      "5       shit blow meclaim faith somebodi still fuck hoe\n",
      "6              sit hate anoth bitch got much shit go on\n",
      "7            caus im tire big bitch come us skinni girl\n",
      "8                  amp might get ya bitch back amp that\n",
      "9                       hobbi includ fight mariam bitch\n",
      "10    keek bitch curv everyon lol walk convers like ...\n",
      "11                           murda gang bitch gang land\n",
      "12                            hoe smoke loser yea go ig\n",
      "13                                 bad bitch thing like\n",
      "14                                            bitch get\n",
      "15                                     bitch nigga miss\n",
      "16                                     bitch plz whatev\n",
      "17                                           bitch love\n",
      "18                             bitch get cut everyday b\n",
      "19                            black bottl amp bad bitch\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#results of pre-process\n",
    "tweet_=preprocess(tweet,special_index,meaningless_index)\n",
    "print(tweet[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.DataFrame(tweet)\n",
    "tweet.to_csv(\"../data/clean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "## 1.LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LIWC on the whole df\n",
    "\n",
    "LIWC_list=[]\n",
    "for i in range(len(X_train)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(X_train[i])\n",
    "    LIWC_list.append(L.readable(labels))\n",
    "LIWC_list\n",
    "'''\n",
    "#update preprocessed text to original df\n",
    "df.tweet=tweet\n",
    "hateSpeech=df[df['hate_speech'] ==3][\"tweet\"].reset_index(drop=True)\n",
    "#it is the list of hate speech\n",
    "LIWC_list1=[]\n",
    "for i in range(len(hateSpeech)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(hateSpeech[i])\n",
    "    LIWC_list1.append(L.readable(labels))\n",
    "LIWC_list1\n",
    "#function that add dictionary\n",
    "def mergeDict(dict1, dict2):\n",
    "   # Merge dictionaries and keep values of common keys in list\n",
    "   dict3 = {**dict1, **dict2}\n",
    "   for key, value in dict3.items():\n",
    "       if key in dict1 and key in dict2:\n",
    "               dict3[key] = value +dict1[key]\n",
    "   return dict3\n",
    "#add those dic together\n",
    "hate_dic=LIWC_list1[0]\n",
    "for i in range(len(LIWC_list1)):\n",
    "    hate_dic=mergeDict(hate_dic, LIWC_list1[i])\n",
    "hate_dic.keys()\n",
    "#find hate_relevent key by analyzing hate speech\n",
    "relevent_key=[k for k, v in hate_dic.items() if v > 50]\n",
    "'''\n",
    "#build dataframe with relevent columns\n",
    "LIWC_df=pd.DataFrame(data=LIWC_list, index=None, columns=None, dtype=None, copy=False)[relevent_key]\n",
    "LIWC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_list=[]\n",
    "for i in range(len(tweet)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(tweet[i])\n",
    "    #I divide the number by length to get fraction of such words in a sentence\n",
    "    LIWC_list.append(L.readable(labels/len(tweet[i])))\n",
    "\n",
    "#LIWC_df=pd.DataFrame(data=LIWC_list, index=None, columns=None, dtype=None, copy=False)\n",
    "#LIWC_df = LIWC_df.fillna(0)\n",
    "#LIWC_df.to_csv(\"LIWC.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIWC_df = LIWC_df.fillna(0)\n",
    "LIWC_df.to_csv(\"../data/LIWC.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data/test data: 22304 2479\n"
     ]
    }
   ],
   "source": [
    "#combine tweet data with labels\n",
    "tweet_pd=pd.DataFrame(tweet)\n",
    "tweet_pd['label']=txt.iloc[:,0]\n",
    "tweet_train, tweet_test = train_test_split(tweet_pd, random_state=17, test_size = 0.1)\n",
    "print(\"size of train data/test data:\",len(tweet_train),len(tweet_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train['label'].to_csv(\"../data/y_train.csv\", index = False)\n",
    "tweet_test['label'].to_csv(\"../data/y_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tweet_train['tweet']\n",
    "X_test = tweet_test['tweet']\n",
    "X_train.to_csv(\"../data/X_train.csv\", index = False)\n",
    "X_test.to_csv(\"../data/X_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Skip-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['frozen pea ibuprofen new best friend crippl',\n",
       "       'day 2014 yank advanc despit 01 loss', 'im go show bitch done'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input data for word2vec\n",
    "tweet_skip_train=np.array(tweet_train.iloc[:,0])\n",
    "tweet_skip_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec by skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7968936, 45330350)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(size=100,min_count=1, workers=5,sg=1)  #dimension is 200, consider 5 words, using skip-gram\n",
    "model.build_vocab(tweet_skip_train)\n",
    "model.train(tweet_skip_train,total_examples = model.corpus_count,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.5338004e-05,\n",
       "  6.90356e-06,\n",
       "  -0.0002092402,\n",
       "  0.000120525394,\n",
       "  -5.929191e-06,\n",
       "  -0.00013927107,\n",
       "  -0.00022716961,\n",
       "  0.00030028514,\n",
       "  0.00016713461,\n",
       "  0.00023493108,\n",
       "  6.502341e-05,\n",
       "  -1.7898315e-05,\n",
       "  0.00014865233,\n",
       "  -7.402255e-05,\n",
       "  -0.0001068102,\n",
       "  -0.00022292548,\n",
       "  -1.3222639e-05,\n",
       "  -0.00028981172,\n",
       "  -0.00012778988,\n",
       "  0.00011876959,\n",
       "  5.852057e-07,\n",
       "  -1.4370815e-05,\n",
       "  -7.430507e-05,\n",
       "  -0.00015036935,\n",
       "  5.8468628e-05,\n",
       "  5.009316e-05,\n",
       "  1.7543732e-06,\n",
       "  -0.000108766355,\n",
       "  -3.526711e-05,\n",
       "  -0.000117424344,\n",
       "  -4.6471865e-05,\n",
       "  -0.00021797811,\n",
       "  0.0002592884,\n",
       "  0.00015103252,\n",
       "  9.7286895e-05,\n",
       "  -0.00018220812,\n",
       "  0.00019661227,\n",
       "  0.00020387181,\n",
       "  -6.972766e-05,\n",
       "  -4.2342806e-05,\n",
       "  0.00027885134,\n",
       "  0.00020340228,\n",
       "  -3.8603557e-05,\n",
       "  -6.758091e-05,\n",
       "  -0.00012057138,\n",
       "  -6.6533874e-05,\n",
       "  0.00010934164,\n",
       "  0.00020706083,\n",
       "  0.00017895704,\n",
       "  9.0009664e-05,\n",
       "  1.1087894e-05,\n",
       "  0.00010911799,\n",
       "  0.000104850405,\n",
       "  3.6064495e-05,\n",
       "  9.6635646e-05,\n",
       "  4.7587193e-07,\n",
       "  -6.564465e-05,\n",
       "  -0.0001336667,\n",
       "  -0.00024683724,\n",
       "  5.6856374e-05,\n",
       "  -1.1039737e-05,\n",
       "  0.00012637503,\n",
       "  -9.844094e-05,\n",
       "  -3.2814478e-05,\n",
       "  -0.00042094142,\n",
       "  -0.00022942405,\n",
       "  -0.00021290222,\n",
       "  0.00027175178,\n",
       "  0.00011402104,\n",
       "  0.0001126906,\n",
       "  -0.00021127258,\n",
       "  -0.00015425349,\n",
       "  -8.269308e-05,\n",
       "  0.0002578101,\n",
       "  -2.407552e-05,\n",
       "  5.3185388e-05,\n",
       "  6.493329e-05,\n",
       "  -5.7739903e-06,\n",
       "  -5.233284e-05,\n",
       "  1.942405e-05,\n",
       "  -0.00025262,\n",
       "  -6.328256e-05,\n",
       "  -0.00012702496,\n",
       "  9.976653e-05,\n",
       "  0.00024830783,\n",
       "  -0.00013913354,\n",
       "  -0.00011040501,\n",
       "  0.00023018595,\n",
       "  -9.139627e-05,\n",
       "  -0.000113954986,\n",
       "  -3.9052364e-05,\n",
       "  2.6204283e-05,\n",
       "  0.00017668743,\n",
       "  -9.944262e-05,\n",
       "  0.00018368236,\n",
       "  0.00014017745,\n",
       "  0.00026027154,\n",
       "  -0.00012735344,\n",
       "  -0.00021649488,\n",
       "  0.00012031146],\n",
       " [-5.9363474e-06,\n",
       "  1.356027e-05,\n",
       "  -0.0001934597,\n",
       "  0.00010789375,\n",
       "  -1.5772941e-05,\n",
       "  -0.00020716024,\n",
       "  -0.00018792825,\n",
       "  0.00023154594,\n",
       "  0.00015803592,\n",
       "  0.00020945797,\n",
       "  5.463841e-05,\n",
       "  4.1617484e-05,\n",
       "  0.00014077044,\n",
       "  -4.704365e-05,\n",
       "  -0.00012286149,\n",
       "  -0.0001932771,\n",
       "  -1.6100785e-05,\n",
       "  -0.00026965557,\n",
       "  -0.000106008774,\n",
       "  0.00010441189,\n",
       "  -2.4146877e-05,\n",
       "  -2.024346e-05,\n",
       "  -2.948208e-05,\n",
       "  -0.00014374556,\n",
       "  7.075943e-05,\n",
       "  5.959789e-05,\n",
       "  8.38128e-06,\n",
       "  -0.00011493129,\n",
       "  -4.0502408e-05,\n",
       "  -0.00010140471,\n",
       "  -3.7099253e-05,\n",
       "  -0.00022023589,\n",
       "  0.00022529539,\n",
       "  0.00013886108,\n",
       "  9.994304e-05,\n",
       "  -0.00016123724,\n",
       "  0.00017323448,\n",
       "  0.00016231473,\n",
       "  -5.5856955e-05,\n",
       "  -3.777878e-05,\n",
       "  0.00024565746,\n",
       "  0.0001848848,\n",
       "  -3.1531603e-05,\n",
       "  -4.8395334e-05,\n",
       "  -0.00010341697,\n",
       "  -6.054997e-05,\n",
       "  9.723325e-05,\n",
       "  0.0001907649,\n",
       "  0.00016480983,\n",
       "  7.8940866e-05,\n",
       "  2.323061e-05,\n",
       "  0.00010083634,\n",
       "  9.3507515e-05,\n",
       "  3.0985422e-05,\n",
       "  9.3917646e-05,\n",
       "  -8.1551e-06,\n",
       "  -4.953815e-05,\n",
       "  -0.00012580224,\n",
       "  -0.00020865582,\n",
       "  4.267294e-05,\n",
       "  -1.8854456e-05,\n",
       "  9.6544514e-05,\n",
       "  -5.9932667e-05,\n",
       "  -6.5273234e-05,\n",
       "  -0.00038398415,\n",
       "  -0.00023269877,\n",
       "  -0.00020109859,\n",
       "  0.00023638905,\n",
       "  9.474061e-05,\n",
       "  9.601427e-05,\n",
       "  -0.00020190472,\n",
       "  -0.00019236183,\n",
       "  -9.203367e-05,\n",
       "  0.00023274859,\n",
       "  -1.1598705e-05,\n",
       "  3.8160393e-05,\n",
       "  7.0166396e-05,\n",
       "  8.764044e-06,\n",
       "  -5.717004e-05,\n",
       "  3.4118766e-05,\n",
       "  -0.00023254097,\n",
       "  -8.971397e-05,\n",
       "  -0.00014683296,\n",
       "  8.843821e-05,\n",
       "  0.00024143362,\n",
       "  -0.00011449385,\n",
       "  -0.000104026425,\n",
       "  0.00019649888,\n",
       "  -8.62288e-05,\n",
       "  -9.591887e-05,\n",
       "  -2.4153005e-06,\n",
       "  4.0697665e-05,\n",
       "  0.00013958706,\n",
       "  -7.63491e-05,\n",
       "  0.00016820167,\n",
       "  0.00014259077,\n",
       "  0.0001770358,\n",
       "  -0.00011438209,\n",
       "  -0.00012622922,\n",
       "  0.00011307213],\n",
       " [-4.9098503e-06,\n",
       "  5.4053635e-05,\n",
       "  -0.0001985202,\n",
       "  0.0001352332,\n",
       "  -7.5498247e-06,\n",
       "  -0.00015797459,\n",
       "  -0.0001853687,\n",
       "  0.00020920014,\n",
       "  0.00012644735,\n",
       "  0.00017718633,\n",
       "  5.979477e-05,\n",
       "  4.0271458e-05,\n",
       "  0.00011960659,\n",
       "  -5.1139144e-05,\n",
       "  -0.00011813831,\n",
       "  -0.00015422491,\n",
       "  -1.1105437e-06,\n",
       "  -0.00018925112,\n",
       "  -9.2341565e-05,\n",
       "  7.975668e-05,\n",
       "  -7.0583746e-06,\n",
       "  -3.5429344e-05,\n",
       "  -3.3085194e-05,\n",
       "  -0.00011845273,\n",
       "  3.1053303e-05,\n",
       "  2.403437e-05,\n",
       "  3.4089837e-05,\n",
       "  -8.025796e-05,\n",
       "  -3.8340375e-05,\n",
       "  -0.00011210307,\n",
       "  -4.95688e-05,\n",
       "  -0.00018232428,\n",
       "  0.00017138528,\n",
       "  0.00013580135,\n",
       "  4.3587974e-05,\n",
       "  -0.00013815721,\n",
       "  0.00016716818,\n",
       "  7.1152586e-05,\n",
       "  -7.2892486e-05,\n",
       "  -3.4916593e-05,\n",
       "  0.0002621299,\n",
       "  0.00015615953,\n",
       "  -3.0112056e-05,\n",
       "  -4.4082364e-05,\n",
       "  -0.00012020967,\n",
       "  -3.719573e-05,\n",
       "  0.00010972882,\n",
       "  0.00016218562,\n",
       "  0.00014791012,\n",
       "  8.7322514e-05,\n",
       "  -7.845472e-06,\n",
       "  0.00010290049,\n",
       "  9.313568e-05,\n",
       "  4.1406973e-05,\n",
       "  6.535734e-05,\n",
       "  -3.9835668e-05,\n",
       "  -5.9770176e-05,\n",
       "  -0.00013059356,\n",
       "  -0.0001697524,\n",
       "  1.2804771e-05,\n",
       "  -6.5541586e-05,\n",
       "  0.00013551908,\n",
       "  -6.821843e-05,\n",
       "  -4.854926e-05,\n",
       "  -0.00032480684,\n",
       "  -0.00020393703,\n",
       "  -0.0001942844,\n",
       "  0.00021002286,\n",
       "  6.463762e-05,\n",
       "  8.531736e-05,\n",
       "  -0.0001588072,\n",
       "  -0.00015175143,\n",
       "  -5.1691673e-05,\n",
       "  0.00019815846,\n",
       "  -1.5213616e-05,\n",
       "  2.1655353e-05,\n",
       "  5.6019882e-05,\n",
       "  -1.9084779e-05,\n",
       "  -3.11119e-05,\n",
       "  5.0352883e-05,\n",
       "  -0.00018977669,\n",
       "  -6.539253e-05,\n",
       "  -0.00012313403,\n",
       "  8.466921e-05,\n",
       "  0.00016975349,\n",
       "  -6.769431e-05,\n",
       "  -6.870322e-05,\n",
       "  0.00011644171,\n",
       "  -8.1345876e-05,\n",
       "  -8.8823654e-05,\n",
       "  -2.30046e-05,\n",
       "  2.9310979e-05,\n",
       "  0.00015179547,\n",
       "  -6.388756e-05,\n",
       "  0.00013887566,\n",
       "  0.00011268287,\n",
       "  0.00014862248,\n",
       "  -0.00010283576,\n",
       "  -0.000109963345,\n",
       "  9.683402e-05],\n",
       " [2.403825e-05,\n",
       "  1.3900144e-05,\n",
       "  -7.190735e-05,\n",
       "  2.6197757e-05,\n",
       "  -1.1114755e-05,\n",
       "  -4.7791054e-05,\n",
       "  -5.0481874e-05,\n",
       "  8.0909114e-05,\n",
       "  4.973005e-05,\n",
       "  6.0129605e-05,\n",
       "  9.142604e-06,\n",
       "  -1.8246269e-06,\n",
       "  4.238967e-05,\n",
       "  -1.6497552e-05,\n",
       "  -3.8767925e-05,\n",
       "  -7.3237126e-05,\n",
       "  -4.4067107e-07,\n",
       "  -7.79684e-05,\n",
       "  -2.9875991e-05,\n",
       "  3.412889e-05,\n",
       "  1.5160325e-06,\n",
       "  -7.858841e-06,\n",
       "  -2.3368044e-05,\n",
       "  -3.1681393e-05,\n",
       "  -1.3229474e-06,\n",
       "  2.465767e-06,\n",
       "  1.0195942e-06,\n",
       "  -2.8601537e-05,\n",
       "  -7.2085477e-06,\n",
       "  -3.7698297e-05,\n",
       "  -1.9550695e-05,\n",
       "  -7.455199e-05,\n",
       "  7.240756e-05,\n",
       "  5.178541e-05,\n",
       "  2.6444972e-05,\n",
       "  -4.793979e-05,\n",
       "  5.6003566e-05,\n",
       "  8.180118e-05,\n",
       "  -3.0319117e-05,\n",
       "  -2.42344e-05,\n",
       "  9.787508e-05,\n",
       "  5.039762e-05,\n",
       "  9.556328e-08,\n",
       "  -1.9782054e-05,\n",
       "  -4.5650802e-05,\n",
       "  -2.7214113e-05,\n",
       "  2.0221807e-05,\n",
       "  5.8179692e-05,\n",
       "  5.7828023e-05,\n",
       "  2.6400832e-05,\n",
       "  3.395374e-07,\n",
       "  3.1358417e-05,\n",
       "  2.9091676e-05,\n",
       "  -1.4274865e-05,\n",
       "  2.808118e-05,\n",
       "  -9.959039e-06,\n",
       "  -1.4119147e-05,\n",
       "  -4.509986e-05,\n",
       "  -7.800372e-05,\n",
       "  1.3246206e-05,\n",
       "  -1.1690123e-05,\n",
       "  5.3198524e-05,\n",
       "  -3.9305305e-05,\n",
       "  -1.860089e-05,\n",
       "  -0.00012352772,\n",
       "  -8.214651e-05,\n",
       "  -5.29789e-05,\n",
       "  6.22367e-05,\n",
       "  3.073965e-05,\n",
       "  3.26915e-05,\n",
       "  -5.880044e-05,\n",
       "  -5.0301875e-05,\n",
       "  -1.8615428e-05,\n",
       "  8.383548e-05,\n",
       "  -1.1938112e-05,\n",
       "  1.4667089e-05,\n",
       "  1.903699e-05,\n",
       "  6.610522e-06,\n",
       "  -2.3723185e-05,\n",
       "  2.6355056e-05,\n",
       "  -7.276339e-05,\n",
       "  -8.497744e-06,\n",
       "  -2.5153218e-05,\n",
       "  2.1369435e-05,\n",
       "  7.149368e-05,\n",
       "  -4.4815308e-05,\n",
       "  -1.8332048e-05,\n",
       "  5.6933543e-05,\n",
       "  -3.3552522e-05,\n",
       "  -3.512029e-05,\n",
       "  -1.4112725e-05,\n",
       "  8.75944e-06,\n",
       "  4.31606e-05,\n",
       "  -3.699349e-05,\n",
       "  5.194974e-05,\n",
       "  4.1245585e-05,\n",
       "  7.729608e-05,\n",
       "  -5.407777e-05,\n",
       "  -6.142387e-05,\n",
       "  7.909978e-05],\n",
       " [3.3860222e-05,\n",
       "  6.0900784e-05,\n",
       "  -0.00024116364,\n",
       "  0.00013697613,\n",
       "  -1.122825e-05,\n",
       "  -0.00017334353,\n",
       "  -0.00021889043,\n",
       "  0.00022410827,\n",
       "  0.00014440808,\n",
       "  0.0002244781,\n",
       "  6.135029e-05,\n",
       "  5.118638e-05,\n",
       "  0.00015258133,\n",
       "  -5.7341902e-05,\n",
       "  -0.00010513535,\n",
       "  -0.0001777128,\n",
       "  3.3057066e-05,\n",
       "  -0.0002053348,\n",
       "  -0.000103572944,\n",
       "  9.2420945e-05,\n",
       "  -8.625672e-07,\n",
       "  -4.3430176e-05,\n",
       "  -4.3428507e-05,\n",
       "  -0.00013138718,\n",
       "  2.046086e-05,\n",
       "  4.0995947e-05,\n",
       "  2.8990635e-05,\n",
       "  -8.948617e-05,\n",
       "  -3.163499e-05,\n",
       "  -0.00013418017,\n",
       "  -5.892792e-05,\n",
       "  -0.00018576033,\n",
       "  0.00021934246,\n",
       "  0.00015494981,\n",
       "  5.768693e-05,\n",
       "  -0.00017606188,\n",
       "  0.00015403554,\n",
       "  0.00014877254,\n",
       "  -8.272428e-05,\n",
       "  -6.0275575e-05,\n",
       "  0.00025477074,\n",
       "  0.00018597765,\n",
       "  -3.1810472e-05,\n",
       "  -5.0855644e-05,\n",
       "  -0.00013214875,\n",
       "  -3.9985833e-05,\n",
       "  9.7619806e-05,\n",
       "  0.00016351746,\n",
       "  0.00016803281,\n",
       "  8.764103e-05,\n",
       "  1.6736827e-05,\n",
       "  9.2361355e-05,\n",
       "  0.000101556056,\n",
       "  1.6772432e-05,\n",
       "  8.674249e-05,\n",
       "  -1.9714636e-05,\n",
       "  -5.126684e-05,\n",
       "  -0.00014591275,\n",
       "  -0.00019076471,\n",
       "  4.0763185e-05,\n",
       "  -8.211406e-05,\n",
       "  0.00014914651,\n",
       "  -8.0996724e-05,\n",
       "  -5.983496e-05,\n",
       "  -0.00032928606,\n",
       "  -0.00022018135,\n",
       "  -0.00018770373,\n",
       "  0.00022277709,\n",
       "  6.200617e-05,\n",
       "  9.448093e-05,\n",
       "  -0.0001883622,\n",
       "  -0.00016812452,\n",
       "  -5.754031e-05,\n",
       "  0.00021977563,\n",
       "  -7.3247966e-06,\n",
       "  8.095463e-06,\n",
       "  6.0136244e-05,\n",
       "  -2.8476866e-06,\n",
       "  -4.870413e-05,\n",
       "  5.59842e-05,\n",
       "  -0.00017480469,\n",
       "  -6.933768e-05,\n",
       "  -0.00010675247,\n",
       "  6.8672554e-05,\n",
       "  0.00018991457,\n",
       "  -8.399971e-05,\n",
       "  -8.140963e-05,\n",
       "  0.00014786536,\n",
       "  -5.83115e-05,\n",
       "  -8.978601e-05,\n",
       "  -1.6006103e-05,\n",
       "  2.8256078e-05,\n",
       "  0.00015703774,\n",
       "  -7.761659e-05,\n",
       "  0.00014876589,\n",
       "  0.00011841953,\n",
       "  0.00015164983,\n",
       "  -0.00013046082,\n",
       "  -0.00012233328,\n",
       "  9.931319e-05]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warnings.filterwarnings(\"ignore\")\n",
    "word_vec=[]\n",
    "v_in_dic=[]\n",
    "#average each word vector to get the vector of a single tweet\n",
    "for i in range(len(tweet_skip_train)):\n",
    "    sum1=0\n",
    "    for w in tweet[i]:\n",
    "        try:\n",
    "            sum1+=model[w]\n",
    "            v_in_dic.append(w)\n",
    "        #for out of dictionary vocab use random select in vocabulary word method to generate vectors\n",
    "        except:\n",
    "            random_w=choice(v_in_dic)\n",
    "            sum1+=model[random_w]\n",
    "            continue\n",
    "    avg=sum1/(len(tweet_skip_train))\n",
    "    avg=list(avg)\n",
    "    word_vec.append(avg)\n",
    "#word_vec[:5]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22304"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the format of the output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [-5.9363474e-06  1.3560270e-05 -1.9345969e-04 ... -1.1438209e-04\n",
      "  -1.2622922e-04  1.1307213e-04]\n",
      " ...\n",
      " [ 5.8622296e-05  6.4456355e-05 -2.3561601e-04 ... -1.1445833e-04\n",
      "  -1.3924936e-04  1.2791694e-04]\n",
      " [ 1.3101550e-05  2.2595434e-05 -6.4107742e-05 ... -2.1456435e-05\n",
      "  -2.4978521e-05  2.6791484e-05]\n",
      " [ 3.6837540e-05  1.1829634e-04 -4.5385252e-04 ... -2.3355952e-04\n",
      "  -2.0691885e-04  2.3315978e-04]]\n",
      "(22304, 100)\n"
     ]
    }
   ],
   "source": [
    "word_vec_array=np.array(word_vec)\n",
    "word_vec_l=np.reshape(word_vec_array[0],(1,100))\n",
    "for i in range(len(tweet_skip_train)):\n",
    "    array=np.reshape(word_vec_array[i],(1,100))\n",
    "    word_vec_l=np.r_[word_vec_l,array]\n",
    "print(word_vec_l)\n",
    "word_vec_lt=word_vec_l\n",
    "word_vec_lt=np.delete(word_vec_lt,0,0)\n",
    "print(word_vec_lt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_lt_ = pd.DataFrame(word_vec_lt)\n",
    "word_vec_lt_.to_csv(\"../data/train_ski.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct test vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_skip_test=np.array(tweet_test.iloc[:,0])\n",
    "#average each word vector to get the vector of a single tweet\n",
    "for i in range(len(tweet_test)):\n",
    "    sum1=0\n",
    "    for w in tweet[i]:\n",
    "        try:\n",
    "            sum1+=model[w]\n",
    "            v_in_dic.append(w)\n",
    "        #for out of dictionary vocab use random select in vocabulary word method to generate vectors\n",
    "        except:\n",
    "            random_w=choice(v_in_dic)\n",
    "            sum1+=model[random_w]\n",
    "            continue\n",
    "    avg=sum1/(len(tweet_skip_train))\n",
    "    avg=list(avg)\n",
    "    word_vec.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [-5.9363474e-06  1.3560270e-05 -1.9345969e-04 ... -1.1438209e-04\n",
      "  -1.2622922e-04  1.1307213e-04]\n",
      " ...\n",
      " [-9.5239329e-06  9.1593240e-05 -3.1487900e-04 ... -2.3024407e-04\n",
      "  -2.6830297e-04  1.9702339e-04]\n",
      " [ 2.3259687e-05  9.7768589e-06 -1.3366160e-04 ... -8.4095700e-05\n",
      "  -8.0500751e-05  8.7899869e-05]\n",
      " [ 1.4624413e-05  1.3782654e-05 -3.8958213e-05 ... -7.3622091e-06\n",
      "   5.6125318e-06  2.0170601e-06]]\n",
      "(2479, 100)\n"
     ]
    }
   ],
   "source": [
    "word_vec_array=np.array(word_vec)\n",
    "word_vec_l=np.reshape(word_vec_array[0],(1,100))\n",
    "for i in range(len(tweet_skip_test)):\n",
    "    array=np.reshape(word_vec_array[i],(1,100))\n",
    "    word_vec_l=np.r_[word_vec_l,array]\n",
    "print(word_vec_l)\n",
    "word_vec_lt=word_vec_l\n",
    "word_vec_lt=np.delete(word_vec_lt,0,0)\n",
    "print(word_vec_lt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_lt_ = pd.DataFrame(word_vec_lt)\n",
    "word_vec_lt_.to_csv(\"../data/test_ski.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save this array as csv\n",
    "\n",
    "f = open('../data/word2vec_skip.csv', 'w')\n",
    "a = csv.writer(f)\n",
    "a.writerows(word_vec_lt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('../data/word2vec_skip1.csv', word_vec_lt, delimiter = ',')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.LDA\n",
    "\n",
    "construct dic for LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a449c61a332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         logger.info(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "tweet = np.array(tweet_train.iloc[:,0])\n",
    "dictionary = corpora.Dictionary(tweet)\n",
    "corpus = [dictionary.doc2bow(words) for words in tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.39856848738008344\n",
      "\n",
      "Coherence Score:  0.4063903418840271\n",
      "\n",
      "Coherence Score:  0.40174023517653634\n",
      "\n",
      "Coherence Score:  0.40982003924347604\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 60, 70, 80]:\n",
    "    lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=i)\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=tweet, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence Score is always below 0.55, so it might not be a good feature extraction method for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, '0.253*\"yo\" + 0.097*\"bitch\" + 0.085*\"ass\" + 0.044*\"smell\" + 0.044*\"like\"')\n",
      "(27, '0.198*\"littl\" + 0.168*\"feel\" + 0.100*\"bitch\" + 0.029*\"ass\" + 0.023*\"like\"')\n",
      "(2, '0.233*\"nigga\" + 0.139*\"hoe\" + 0.073*\"niggah\" + 0.067*\"n\" + 0.056*\"bitch\"')\n",
      "(42, '0.121*\"bitch\" + 0.112*\"kill\" + 0.089*\"way\" + 0.069*\"pleas\" + 0.062*\"alreadi\"')\n",
      "(48, '0.203*\"love\" + 0.134*\"u\" + 0.091*\"bitch\" + 0.068*\"cuz\" + 0.057*\"hoe\"')\n",
      "(56, '0.133*\"tell\" + 0.100*\"turn\" + 0.093*\"boy\" + 0.064*\"bitch\" + 0.044*\"dyke\"')\n",
      "(29, '0.105*\"watch\" + 0.097*\"ask\" + 0.097*\"made\" + 0.085*\"could\" + 0.072*\"bitch\"')\n",
      "(63, '0.198*\"would\" + 0.068*\"thought\" + 0.059*\"cracker\" + 0.038*\"kick\" + 0.037*\"redneck\"')\n",
      "(54, '0.093*\"peopl\" + 0.084*\"guy\" + 0.078*\"say\" + 0.052*\"bitch\" + 0.044*\"im\"')\n",
      "(45, '0.154*\"me\" + 0.129*\"bitch\" + 0.104*\"like\" + 0.083*\"start\" + 0.059*\"act\"')\n",
      "(53, '0.154*\"even\" + 0.095*\"kid\" + 0.093*\"he\" + 0.048*\"happi\" + 0.042*\"word\"')\n",
      "(57, '0.090*\"last\" + 0.089*\"night\" + 0.075*\"care\" + 0.058*\"him\" + 0.047*\"whole\"')\n",
      "(18, '0.212*\"think\" + 0.052*\"mock\" + 0.043*\"speak\" + 0.040*\"long\" + 0.038*\"bitch\"')\n",
      "(28, '0.097*\"dick\" + 0.074*\"damn\" + 0.065*\"better\" + 0.057*\"game\" + 0.049*\"bitch\"')\n",
      "(44, '0.195*\"make\" + 0.096*\"fuckin\" + 0.065*\"bitch\" + 0.058*\"smh\" + 0.055*\"god\"')\n",
      "(10, '0.139*\"right\" + 0.089*\"bitch\" + 0.065*\"much\" + 0.055*\"sound\" + 0.049*\"basic\"')\n",
      "(15, '0.305*\"trash\" + 0.085*\"old\" + 0.074*\"hair\" + 0.045*\"great\" + 0.032*\"bitch\"')\n",
      "(22, '0.178*\"hate\" + 0.099*\"bitch\" + 0.066*\"gay\" + 0.036*\"hand\" + 0.036*\"bruh\"')\n",
      "(21, '0.218*\"that\" + 0.101*\"nigger\" + 0.087*\"ghetto\" + 0.059*\"well\" + 0.055*\"follow\"')\n",
      "(8, '0.185*\"real\" + 0.104*\"stupid\" + 0.079*\"bitch\" + 0.058*\"thank\" + 0.051*\"best\"')\n",
      "(array([[0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       ...,\n",
      "       [2.3173292 , 0.01428571, 1.4679486 , ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571]], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# lda model, num_topics is the number of topic\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=70)\n",
    "# print 5 words for each topic \n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic)\n",
    "# topic infer\n",
    "lda_infer= lda.inference(corpus)\n",
    "print(lda.inference(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 70)\n"
     ]
    }
   ],
   "source": [
    "print(lda_infer[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_infer = pd.DataFrame(lda_infer[0])\n",
    "lda_infer.to_csv(\"../data/lda_infer.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv('../data/clean.csv')['tweet']\n",
    "X_train=pd.read_csv(\"../data/X_train.csv\")['tweet']\n",
    "X_test=pd.read_csv(\"../data/X_test.csv\")['tweet']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = X_train.values\n",
    "text_ = X_test.values\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
    "vectorizer.fit(text)\n",
    "tfidf_train = vectorizer.transform(text)\n",
    "tfidf_test =vectorizer.transform(text_)\n",
    "tfidf_train = pd.DataFrame.sparse.from_spmatrix(tfidf_train)\n",
    "tfidf_test = pd.DataFrame.sparse.from_spmatrix(tfidf_test)\n",
    "#tfidf_train = pd.DataFrame(tfidf_train)\n",
    "#tfidf_train.shape\n",
    "\n",
    "#tfidf_test = pd.DataFrame(tfidf_test)\n",
    "tfidf_train.to_csv(\"../data/tfidf_train.csv\", index = False)\n",
    "tfidf_test.to_csv(\"../data/tfidf_test.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
