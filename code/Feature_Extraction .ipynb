{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "from random import choice\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import csv\n",
    "from liwc import LIWC\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  hate_speech  offensive_language  neither  class  \\\n",
      "0      3            0                   0        3      2   \n",
      "1      3            0                   3        0      1   \n",
      "2      3            0                   3        0      1   \n",
      "3      3            0                   2        1      1   \n",
      "4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "(24783, 6)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/labeled_data.csv\")\n",
    "df.drop(df.columns[[0]], axis=1,inplace=True)\n",
    "print(df.iloc[:5,:])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1     !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2     !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3     !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4     !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "5     !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
       "6     !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
       "7     !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
       "8     \" &amp; you might not get ya bitch back &amp; ...\n",
       "9     \" @rhythmixx_ :hobbies include: fighting Maria...\n",
       "10    \" Keeks is a bitch she curves everyone \" lol I...\n",
       "11                   \" Murda Gang bitch its Gang Land \"\n",
       "12    \" So hoes that smoke are losers ? \" yea ... go...\n",
       "13        \" bad bitches is the only thing that i like \"\n",
       "14                              \" bitch get up off me \"\n",
       "15                      \" bitch nigga miss me with it \"\n",
       "16                               \" bitch plz whatever \"\n",
       "17                            \" bitch who do you love \"\n",
       "18                   \" bitches get cut off everyday B \"\n",
       "19                   \" black bottle &amp; a bad bitch \"\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Yankees\n"
     ]
    }
   ],
   "source": [
    "# Index of tweet only contain format like\"#XX #XX\" (which would be empty after remove hashtag)\n",
    "# We need to specially deal with them\n",
    "special_index=[804,826,846,848,849,923,1016,1122,1909,3398,4818,5711,6098,6279,6332,6668,7168,11951,15859,18062]\n",
    "# for example\n",
    "print(df['tweet'][804])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TNKidsFoodPorn @Oreo ..............&#128530;\n",
      "@hoes &#9829;\n"
     ]
    }
   ],
   "source": [
    "# Also there are two meaningless tweet\n",
    "print(df['tweet'][4828])\n",
    "print(df['tweet'][6098])\n",
    "meaningless_index=[4828,6098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy original text\n",
    "txt=df.iloc[:,4:6]\n",
    "tweet=txt.iloc[:,1].copy(deep=True)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet_,specail_index,meaningless_index):\n",
    "    #remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    other = [\"#ff\", \"ff\", \"rt\"]\n",
    "    stopwords.extend(other)\n",
    "    #word tokenize and remove stopwords\n",
    "    \n",
    "    for i in range(tweet_.shape[0]):\n",
    "        text=tweet_[i].lower()\n",
    "        text1=''.join([word+\" \" for word in text.split() if word not in stopwords])\n",
    "        tweet_[i]=text1\n",
    "    \n",
    "    #remove digit and excessive whitespace\n",
    "    #remove url mention and hashtag\n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    for i in range(tweet_.shape[0]):\n",
    "        text_string=tweet_[i]\n",
    "        parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "        parsed_text = re.sub(mention_regex, '', parsed_text) \n",
    "        if i not in specail_index:\n",
    "            parsed_text = re.sub(hashtag_regex, '', parsed_text)\n",
    "        else:\n",
    "            parsed_text = re.sub('#','',parsed_text)\n",
    "        if i in meaningless_index:\n",
    "            parsed_text ='This is a simple tweet'\n",
    "        tweet_[i]=parsed_text\n",
    "    \n",
    "    #tokenize and stemming\n",
    "    \"\"\"Removes punctuation and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    for i in range(tweet_.shape[0]):\n",
    "        \n",
    "        tweet_stem=tweet_[i]\n",
    "        \n",
    "        remove = str.maketrans('','',string.punctuation) \n",
    "        tweet_stem = tweet_stem.translate(remove).strip()\n",
    "\n",
    "        tweet_process = [stemmer.stem(t) for t in tweet_stem.split()]\n",
    "        \n",
    "        #tweet_[i]=tweet_process\n",
    "        \n",
    "        tweet_[i] = ' '.join([str(elem) for elem in tweet_process]) \n",
    "        \n",
    "\n",
    "        \n",
    "    return tweet_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     woman complain clean hous amp man alway take t...\n",
      "1     boy dat coldtyga dwn bad cuffin dat hoe 1st place\n",
      "2            dawg ever fuck bitch start cri confus shit\n",
      "3                                      look like tranni\n",
      "4        shit hear might true might faker bitch told ya\n",
      "5       shit blow meclaim faith somebodi still fuck hoe\n",
      "6              sit hate anoth bitch got much shit go on\n",
      "7            caus im tire big bitch come us skinni girl\n",
      "8                  amp might get ya bitch back amp that\n",
      "9                       hobbi includ fight mariam bitch\n",
      "10    keek bitch curv everyon lol walk convers like ...\n",
      "11                           murda gang bitch gang land\n",
      "12                            hoe smoke loser yea go ig\n",
      "13                                 bad bitch thing like\n",
      "14                                            bitch get\n",
      "15                                     bitch nigga miss\n",
      "16                                     bitch plz whatev\n",
      "17                                           bitch love\n",
      "18                             bitch get cut everyday b\n",
      "19                            black bottl amp bad bitch\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#results of pre-process\n",
    "tweet_=preprocess(tweet,special_index,meaningless_index)\n",
    "print(tweet[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.DataFrame(tweet)\n",
    "tweet.to_csv(\"../data/clean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "## 1.LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LIWC on the whole df\n",
    "\n",
    "LIWC_list=[]\n",
    "for i in range(len(X_train)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(X_train[i])\n",
    "    LIWC_list.append(L.readable(labels))\n",
    "LIWC_list\n",
    "'''\n",
    "#update preprocessed text to original df\n",
    "df.tweet=tweet\n",
    "hateSpeech=df[df['hate_speech'] ==3][\"tweet\"].reset_index(drop=True)\n",
    "#it is the list of hate speech\n",
    "LIWC_list1=[]\n",
    "for i in range(len(hateSpeech)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(hateSpeech[i])\n",
    "    LIWC_list1.append(L.readable(labels))\n",
    "LIWC_list1\n",
    "#function that add dictionary\n",
    "def mergeDict(dict1, dict2):\n",
    "   # Merge dictionaries and keep values of common keys in list\n",
    "   dict3 = {**dict1, **dict2}\n",
    "   for key, value in dict3.items():\n",
    "       if key in dict1 and key in dict2:\n",
    "               dict3[key] = value +dict1[key]\n",
    "   return dict3\n",
    "#add those dic together\n",
    "hate_dic=LIWC_list1[0]\n",
    "for i in range(len(LIWC_list1)):\n",
    "    hate_dic=mergeDict(hate_dic, LIWC_list1[i])\n",
    "hate_dic.keys()\n",
    "#find hate_relevent key by analyzing hate speech\n",
    "relevent_key=[k for k, v in hate_dic.items() if v > 50]\n",
    "'''\n",
    "#build dataframe with relevent columns\n",
    "LIWC_df=pd.DataFrame(data=LIWC_list, index=None, columns=None, dtype=None, copy=False)[relevent_key]\n",
    "LIWC_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIWC_list=[]\n",
    "for i in range(len(tweet)):\n",
    "    L = LIWC()\n",
    "    length, matched, labels = L.classify(tweet[i])\n",
    "    #I divide the number by length to get fraction of such words in a sentence\n",
    "    LIWC_list.append(L.readable(labels/len(tweet[i])))\n",
    "\n",
    "#LIWC_df=pd.DataFrame(data=LIWC_list, index=None, columns=None, dtype=None, copy=False)\n",
    "#LIWC_df = LIWC_df.fillna(0)\n",
    "#LIWC_df.to_csv(\"LIWC.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIWC_df = LIWC_df.fillna(0)\n",
    "LIWC_df.to_csv(\"../data/LIWC.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data/test data: 22304 2479\n"
     ]
    }
   ],
   "source": [
    "#combine tweet data with labels\n",
    "tweet_pd=pd.DataFrame(tweet)\n",
    "tweet_pd['label']=txt.iloc[:,0]\n",
    "tweet_train, tweet_test = train_test_split(tweet_pd, random_state=17, test_size = 0.1)\n",
    "print(\"size of train data/test data:\",len(tweet_train),len(tweet_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train['label'].to_csv(\"../data/y_train.csv\", index = False)\n",
    "tweet_test['label'].to_csv(\"../data/y_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tweet_train['tweet']\n",
    "X_test = tweet_test['tweet']\n",
    "X_train.to_csv(\"../data/X_train.csv\", index = False)\n",
    "X_test.to_csv(\"../data/X_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Skip-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['frozen pea ibuprofen new best friend crippl',\n",
       "       'day 2014 yank advanc despit 01 loss', 'im go show bitch done'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input data for word2vec\n",
    "tweet_skip_train=np.array(tweet_train.iloc[:,0])\n",
    "tweet_skip_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec by skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7974372, 45307100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(size=100,min_count=1, workers=5,sg=1)  #dimension is 200, consider 5 words, using skip-gram\n",
    "model.build_vocab(tweet_skip_train)\n",
    "model.train(tweet_skip_train,total_examples = model.corpus_count,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-820037785a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_skip_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msum1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0msum1\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#warnings.filterwarnings(\"ignore\")\n",
    "word_vec=[]\n",
    "v_in_dic=[]\n",
    "#average each word vector to get the vector of a single tweet\n",
    "for i in range(len(tweet_skip_train)):\n",
    "    sum1=0\n",
    "    for w in tweet[i]:\n",
    "        try:\n",
    "            sum1+=model[w]\n",
    "            v_in_dic.append(w)\n",
    "        #for out of dictionary vocab use random select in vocabulary word method to generate vectors\n",
    "        except:\n",
    "            random_w=choice(v_in_dic)\n",
    "            sum1+=model[random_w]\n",
    "            continue\n",
    "    avg=sum1/(len(tweet_skip_train))\n",
    "    avg=list(avg)\n",
    "    word_vec.append(avg)\n",
    "#word_vec[:5]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22304"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the format of the output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [-5.9363474e-06  1.3560270e-05 -1.9345969e-04 ... -1.1438209e-04\n",
      "  -1.2622922e-04  1.1307213e-04]\n",
      " ...\n",
      " [ 5.8622296e-05  6.4456355e-05 -2.3561601e-04 ... -1.1445833e-04\n",
      "  -1.3924936e-04  1.2791694e-04]\n",
      " [ 1.3101550e-05  2.2595434e-05 -6.4107742e-05 ... -2.1456435e-05\n",
      "  -2.4978521e-05  2.6791484e-05]\n",
      " [ 3.6837540e-05  1.1829634e-04 -4.5385252e-04 ... -2.3355952e-04\n",
      "  -2.0691885e-04  2.3315978e-04]]\n",
      "(22304, 100)\n"
     ]
    }
   ],
   "source": [
    "word_vec_array=np.array(word_vec)\n",
    "word_vec_l=np.reshape(word_vec_array[0],(1,100))\n",
    "for i in range(len(tweet_skip_train)):\n",
    "    array=np.reshape(word_vec_array[i],(1,100))\n",
    "    word_vec_l=np.r_[word_vec_l,array]\n",
    "#print(word_vec_l)\n",
    "word_vec_lt=word_vec_l\n",
    "word_vec_lt=np.delete(word_vec_lt,0,0)\n",
    "print(word_vec_lt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_lt_ = pd.DataFrame(word_vec_lt)\n",
    "word_vec_lt_.to_csv(\"../data/train_ski.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct test vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_skip_test=np.array(tweet_test.iloc[:,0])\n",
    "#average each word vector to get the vector of a single tweet\n",
    "for i in range(len(tweet_test)):\n",
    "    sum1=0\n",
    "    for w in tweet[i]:\n",
    "        try:\n",
    "            sum1+=model[w]\n",
    "            v_in_dic.append(w)\n",
    "        #for out of dictionary vocab use random select in vocabulary word method to generate vectors\n",
    "        except:\n",
    "            random_w=choice(v_in_dic)\n",
    "            sum1+=model[random_w]\n",
    "            continue\n",
    "    avg=sum1/(len(tweet_skip_train))\n",
    "    avg=list(avg)\n",
    "    word_vec.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [ 8.5338004e-05  6.9035600e-06 -2.0924020e-04 ... -1.2735344e-04\n",
      "  -2.1649488e-04  1.2031146e-04]\n",
      " [-5.9363474e-06  1.3560270e-05 -1.9345969e-04 ... -1.1438209e-04\n",
      "  -1.2622922e-04  1.1307213e-04]\n",
      " ...\n",
      " [-9.5239329e-06  9.1593240e-05 -3.1487900e-04 ... -2.3024407e-04\n",
      "  -2.6830297e-04  1.9702339e-04]\n",
      " [ 2.3259687e-05  9.7768589e-06 -1.3366160e-04 ... -8.4095700e-05\n",
      "  -8.0500751e-05  8.7899869e-05]\n",
      " [ 1.4624413e-05  1.3782654e-05 -3.8958213e-05 ... -7.3622091e-06\n",
      "   5.6125318e-06  2.0170601e-06]]\n",
      "(2479, 100)\n"
     ]
    }
   ],
   "source": [
    "word_vec_array=np.array(word_vec)\n",
    "word_vec_l=np.reshape(word_vec_array[0],(1,100))\n",
    "for i in range(len(tweet_skip_test)):\n",
    "    array=np.reshape(word_vec_array[i],(1,100))\n",
    "    word_vec_l=np.r_[word_vec_l,array]\n",
    "#print(word_vec_l)\n",
    "word_vec_lt=word_vec_l\n",
    "word_vec_lt=np.delete(word_vec_lt,0,0)\n",
    "print(word_vec_lt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_lt_ = pd.DataFrame(word_vec_lt)\n",
    "word_vec_lt_.to_csv(\"../data/test_ski.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save this array as csv\n",
    "\n",
    "f = open('../data/word2vec_skip.csv', 'w')\n",
    "a = csv.writer(f)\n",
    "a.writerows(word_vec_lt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('../data/word2vec_skip1.csv', word_vec_lt, delimiter = ',')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.LDA\n",
    "\n",
    "construct dic for LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a449c61a332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         logger.info(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "tweet = np.array(tweet_train.iloc[:,0])\n",
    "dictionary = corpora.Dictionary(tweet)\n",
    "corpus = [dictionary.doc2bow(words) for words in tweet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.39856848738008344\n",
      "\n",
      "Coherence Score:  0.4063903418840271\n",
      "\n",
      "Coherence Score:  0.40174023517653634\n",
      "\n",
      "Coherence Score:  0.40982003924347604\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 60, 70, 80]:\n",
    "    lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=i)\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=tweet, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence Score is always below 0.55, so it might not be a good feature extraction method for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, '0.253*\"yo\" + 0.097*\"bitch\" + 0.085*\"ass\" + 0.044*\"smell\" + 0.044*\"like\"')\n",
      "(27, '0.198*\"littl\" + 0.168*\"feel\" + 0.100*\"bitch\" + 0.029*\"ass\" + 0.023*\"like\"')\n",
      "(2, '0.233*\"nigga\" + 0.139*\"hoe\" + 0.073*\"niggah\" + 0.067*\"n\" + 0.056*\"bitch\"')\n",
      "(42, '0.121*\"bitch\" + 0.112*\"kill\" + 0.089*\"way\" + 0.069*\"pleas\" + 0.062*\"alreadi\"')\n",
      "(48, '0.203*\"love\" + 0.134*\"u\" + 0.091*\"bitch\" + 0.068*\"cuz\" + 0.057*\"hoe\"')\n",
      "(56, '0.133*\"tell\" + 0.100*\"turn\" + 0.093*\"boy\" + 0.064*\"bitch\" + 0.044*\"dyke\"')\n",
      "(29, '0.105*\"watch\" + 0.097*\"ask\" + 0.097*\"made\" + 0.085*\"could\" + 0.072*\"bitch\"')\n",
      "(63, '0.198*\"would\" + 0.068*\"thought\" + 0.059*\"cracker\" + 0.038*\"kick\" + 0.037*\"redneck\"')\n",
      "(54, '0.093*\"peopl\" + 0.084*\"guy\" + 0.078*\"say\" + 0.052*\"bitch\" + 0.044*\"im\"')\n",
      "(45, '0.154*\"me\" + 0.129*\"bitch\" + 0.104*\"like\" + 0.083*\"start\" + 0.059*\"act\"')\n",
      "(53, '0.154*\"even\" + 0.095*\"kid\" + 0.093*\"he\" + 0.048*\"happi\" + 0.042*\"word\"')\n",
      "(57, '0.090*\"last\" + 0.089*\"night\" + 0.075*\"care\" + 0.058*\"him\" + 0.047*\"whole\"')\n",
      "(18, '0.212*\"think\" + 0.052*\"mock\" + 0.043*\"speak\" + 0.040*\"long\" + 0.038*\"bitch\"')\n",
      "(28, '0.097*\"dick\" + 0.074*\"damn\" + 0.065*\"better\" + 0.057*\"game\" + 0.049*\"bitch\"')\n",
      "(44, '0.195*\"make\" + 0.096*\"fuckin\" + 0.065*\"bitch\" + 0.058*\"smh\" + 0.055*\"god\"')\n",
      "(10, '0.139*\"right\" + 0.089*\"bitch\" + 0.065*\"much\" + 0.055*\"sound\" + 0.049*\"basic\"')\n",
      "(15, '0.305*\"trash\" + 0.085*\"old\" + 0.074*\"hair\" + 0.045*\"great\" + 0.032*\"bitch\"')\n",
      "(22, '0.178*\"hate\" + 0.099*\"bitch\" + 0.066*\"gay\" + 0.036*\"hand\" + 0.036*\"bruh\"')\n",
      "(21, '0.218*\"that\" + 0.101*\"nigger\" + 0.087*\"ghetto\" + 0.059*\"well\" + 0.055*\"follow\"')\n",
      "(8, '0.185*\"real\" + 0.104*\"stupid\" + 0.079*\"bitch\" + 0.058*\"thank\" + 0.051*\"best\"')\n",
      "(array([[0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       ...,\n",
      "       [2.3173292 , 0.01428571, 1.4679486 , ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571],\n",
      "       [0.01428571, 0.01428571, 0.01428571, ..., 0.01428571, 0.01428571,\n",
      "        0.01428571]], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# lda model, num_topics is the number of topic\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=70)\n",
    "# print 5 words for each topic \n",
    "for topic in lda.print_topics(num_words=5):\n",
    "    print(topic)\n",
    "# topic infer\n",
    "lda_infer= lda.inference(corpus)\n",
    "print(lda.inference(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 70)\n"
     ]
    }
   ],
   "source": [
    "print(lda_infer[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_infer = pd.DataFrame(lda_infer[0])\n",
    "lda_infer.to_csv(\"../data/lda_infer.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv('../data/clean.csv')['tweet']\n",
    "X_train=pd.read_csv(\"../data/X_train.csv\")['tweet']\n",
    "X_test=pd.read_csv(\"../data/X_test.csv\")['tweet']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = X_train.values\n",
    "text_ = X_test.values\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
    "vectorizer.fit(text)\n",
    "tfidf_train = vectorizer.transform(text)\n",
    "tfidf_test =vectorizer.transform(text_)\n",
    "tfidf_train = pd.DataFrame.sparse.from_spmatrix(tfidf_train)\n",
    "tfidf_test = pd.DataFrame.sparse.from_spmatrix(tfidf_test)\n",
    "#tfidf_train = pd.DataFrame(tfidf_train)\n",
    "#tfidf_train.shape\n",
    "\n",
    "#tfidf_test = pd.DataFrame(tfidf_test)\n",
    "tfidf_train.to_csv(\"../data/tfidf_train.csv\", index = False)\n",
    "tfidf_test.to_csv(\"../data/tfidf_test.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
